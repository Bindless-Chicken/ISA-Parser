# SOP2 Instructions
0	S_ADD_U32	Add two unsigned integers with carry-out. D.u32 = S0.u32 + S1.u32; SCC = S0.u32 + S1.u32 >= 0x100000000ULL ? 1 : 0.
1	S_SUB_U32	Subtract the second unsigned integer from the first with carry-out. D.u = S0.u - S1.u; SCC = (S1.u > S0.u ? 1 : 0). // unsigned overflow or carry-out for S_SUBB_U32.
2	S_ADD_I32	Add two signed integers with carry-out. This opcode is not suitable for use with S_ADDC_U32 for implementing 64-bit operations. D.i = S0.i + S1.i; SCC = (S0.u[31] == S1.u[31] && S0.u[31] != D.u[31]). // signed overflow.
3	S_SUB_I32	Subtract the second signed integer from the first with carry-out. This opcode is not suitable for use with S_SUBB_U32 for implementing 64-bit operations. D.i = S0.i - S1.i; SCC = (S0.u[31] != S1.u[31] && S0.u[31] != D.u[31]). // signed overflow.
4	S_ADDC_U32	Add two unsigned integers with carry-in and carry-out. D.u32 = S0.u32 + S1.u32 + SCC; SCC = S0.u32 + S1.u32 + SCC >= 0x100000000ULL ? 1 : 0.
5	S_SUBB_U32	Subtract the second unsigned integer from the first with carry-in and carry-out. D.u = S0.u - S1.u - SCC; SCC = (S1.u + SCC > S0.u ? 1 : 0). // unsigned overflow.
6	S_MIN_I32	Minimum of two signed integers. D.i = (S0.i < S1.i) ? S0.i : S1.i; SCC = (S0.i < S1.i).
7	S_MIN_U32	Minimum of two unsigned integers. D.u = (S0.u < S1.u) ? S0.u : S1.u; SCC = (S0.u < S1.u).
8	S_MAX_I32	Maximum of two signed integers. D.i = (S0.i > S1.i) ? S0.i : S1.i; SCC = (S0.i > S1.i).
9	S_MAX_U32	Maximum of two unsigned integers. D.u = (S0.u > S1.u) ? S0.u : S1.u; SCC = (S0.u > S1.u).
10	S_CSELECT_B32	Conditional select based on scalar condition code. D.u = SCC ? S0.u : S1.u.
11	S_CSELECT_B64	Conditional select base on scalar condition code. D.u64 = SCC ? S0.u64 : S1.u64.
14	S_AND_B32	Bitwise AND. D = S0 & S1; SCC = (D != 0).
15	S_AND_B64	Bitwise AND. D = S0 & S1; SCC = (D != 0).
16	S_OR_B32	Bitwise OR. D = S0 | S1; SCC = (D != 0).
17	S_OR_B64	Bitwise OR. D = S0 | S1; SCC = (D != 0).
18	S_XOR_B32	Bitwise XOR. D = S0 ^ S1; SCC = (D != 0).
19	S_XOR_B64	Bitwise XOR. D = S0 ^ S1; SCC = (D != 0).
20	S_ANDN2_B32	Bitwise ANDN2. D = S0 & ~S1; SCC = (D != 0).
21	S_ANDN2_B64	Bitwise ANDN2. D = S0 & ~S1; SCC = (D != 0).
22	S_ORN2_B32	Bitwise ORN2. D = S0 | ~S1; SCC = (D != 0).
23	S_ORN2_B64	Bitwise ORN2. D = S0 | ~S1; SCC = (D != 0).
24	S_NAND_B32	Bitwise NAND. D = ~(S0 & S1); SCC = (D != 0).
25	S_NAND_B64	Bitwise NAND. D = ~(S0 & S1); SCC = (D != 0).
26	S_NOR_B32	Bitwise NOR. D = ~(S0 | S1); SCC = (D != 0).
27	S_NOR_B64	Bitwise NOR. D = ~(S0 | S1); SCC = (D != 0).
28	S_XNOR_B32	Bitwise XNOR. D = ~(S0 ^ S1); SCC = (D != 0).
29	S_XNOR_B64	Bitwise XNOR. D = ~(S0 ^ S1); SCC = (D != 0).
30	S_LSHL_B32	Logical shift left. D.u = S0.u << S1.u[4:0]; SCC = (D.u != 0).
31	S_LSHL_B64	Logical shift left. D.u64 = S0.u64 << S1.u[5:0]; SCC = (D.u64 != 0).
32	S_LSHR_B32	Logical shift right. D.u = S0.u >> S1.u[4:0]; SCC = (D.u != 0).
33	S_LSHR_B64	Logical shift right. D.u64 = S0.u64 >> S1.u[5:0]; SCC = (D.u64 != 0).
34	S_ASHR_I32	Arithmetic shift right (preserve sign bit). D.i = signext(S0.i) >> S1.u[4:0]; SCC = (D.i != 0).
35	S_ASHR_I64	Arithmetic shift right (preserve sign bit). D.i64 = signext(S0.i64) >> S1.u[5:0]; SCC = (D.i64 != 0).
36	S_BFM_B32	Bitfield mask. D.u = ((1 << S0.u[4:0]) - 1) << S1.u[4:0].
37	S_BFM_B64	Bitfield mask. D.u64 = ((1ULL << S0.u[5:0]) - 1) << S1.u[5:0].
38	S_MUL_I32	Multiply two signed integers. D.i = S0.i * S1.i.
39	S_BFE_U32	Bit field extract.  S0 is Data, S1[4:0] is field offset, S1[22:16] is field width. D.u = (S0.u >> S1.u[4:0]) & ((1 << S1.u[22:16]) - 1); SCC = (D.u != 0).
40	S_BFE_I32	Bit field extract.  S0 is Data, S1[4:0] is field offset, S1[22:16] is field width. D.i = signext((S0.i >> S1.u[4:0]) & ((1 << S1.u[22:16]) - 1)); SCC = (D.i != 0).
41	S_BFE_U64	Bit field extract.  S0 is Data, S1[5:0] is field offset, S1[22:16] is field width. D.u64 = (S0.u64 >> S1.u[5:0]) & ((1 << S1.u[22:16]) - 1); SCC = (D.u64 != 0).
42	S_BFE_I64	Bit field extract.  S0 is Data, S1[5:0] is field offset, S1[22:16] is field width. D.i64 = signext((S0.i64 >> S1.u[5:0]) & ((1 << S1.u[22:16]) - 1)); SCC = (D.i64 != 0).
44	S_ABSDIFF_I32	Compute the absolute value of difference between two values. D.i = S0.i - S1.i; if(D.i < 0) then D.i = -D.i; endif; SCC = (D.i != 0). Functional examples: S_ABSDIFF_I32(0x00000002, 0x00000005) => 0x00000003 S_ABSDIFF_I32(0xffffffff, 0x00000000) => 0x00000001 S_ABSDIFF_I32(0x80000000, 0x00000000) => 0x80000000// Note: result is negative! S_ABSDIFF_I32(0x80000000, 0x00000001) => 0x7fffffff S_ABSDIFF_I32(0x80000000, 0xffffffff) => 0x7fffffff S_ABSDIFF_I32(0x80000000, 0xfffffffe) => 0x7ffffffe
46	S_LSHL1_ADD_U32	Logical shift left by 1 bit and then add. D.u = (S0.u << N) + S1.u; // N is the shift value in the opcode SCC = (((S0.u << N) + S1.u) >= 0x100000000ULL ? 1 : 0). // unsigned overflow.
47	S_LSHL2_ADD_U32	Logical shift left by 2 bits and then add. D.u = (S0.u << N) + S1.u; // N is the shift value in the opcode SCC = (((S0.u << N) + S1.u) >= 0x100000000ULL ? 1 : 0). // unsigned overflow.
48	S_LSHL3_ADD_U32	Logical shift left by 3 bits and then add. D.u = (S0.u << N) + S1.u; // N is the shift value in the opcode SCC = (((S0.u << N) + S1.u) >= 0x100000000ULL ? 1 : 0). // unsigned overflow.
49	S_LSHL4_ADD_U32	Logical shift left by 4 bits and then add. D.u = (S0.u << N) + S1.u; // N is the shift value in the opcode SCC = (((S0.u << N) + S1.u) >= 0x100000000ULL ? 1 : 0). // unsigned overflow.
50	S_PACK_LL_B32_B16	Pack two short values into the destination. D.u[31:0] = { S1.u[15:0], S0.u[15:0] }.
51	S_PACK_LH_B32_B16	Pack two short values into the destination. D.u[31:0] = { S1.u[31:16], S0.u[15:0] }.
52	S_PACK_HH_B32_B16	Pack two short values into the destination. D.u[31:0] = { S1.u[31:16], S0.u[31:16] }.
53	S_MUL_HI_U32	Multiple two unsigned integers and store the high 32 bits. D.u = (S0.u * S1.u) >> 32.
54	S_MUL_HI_I32	Multiple two signed integers and store the high 32 bits. D.i = (S0.i * S1.i) >> 32.

# SOPK Instructions
0	S_MOVK_I32	Sign extension from a 16-bit constant. D.i32 = signext(SIMM16[15:0]).
1	S_VERSION	Do nothing.  Argument is ignored by hardware.  This opcode is not designed for inserting wait states as it is possible the next instruction will issue in the same cycle.  Do not use this opcode to resolve wait state hazards, use S_NOP instead. This opcode is used to specify the microcode version for tools that interpret shader microcode; it may also be used to validate microcode is running with the correct compatibility settings in drivers and functional models that support multiple generations. We strongly encourage this opcode be included at the top of every shader block to simplify debug and catch configuration errors. This opcode must appear in the first 16 bytes of a block of shader code in order to be recognized by external tools and functional models. Avoid placing opcodes > 32 bits or encodings that are not available in all versions of the microcode before the S_VERSION opcode. If this opcode is absent then tools are allowed to make a 'best guess' of the microcode version using cues from the environment; the guess may be incorrect and lead to an invalid decode. It is highly recommended that this be the FIRST opcode of a shader block except for trap handlers, where it should be the SECOND opcode (allowing the first opcode to be a 32-bit branch to accommodate context switch). SIMM16[7:0] specifies the microcode version. SIMM16[15:8] must be set to zero.
2	S_CMOVK_I32	Conditional move with sign extension. if(SCC) D.i32 = signext(SIMM16[15:0]); endif.
3	S_CMPK_EQ_I32	SCC = (S0.i32 == signext(SIMM16[15:0])).
4	S_CMPK_LG_I32	SCC = (S0.i32 != signext(SIMM16[15:0])).
5	S_CMPK_GT_I32	SCC = (S0.i32 > signext(SIMM16[15:0])).
6	S_CMPK_GE_I32	SCC = (S0.i32 >= signext(SIMM16[15:0])).
7	S_CMPK_LT_I32	SCC = (S0.i32 < signext(SIMM16[15:0])).
8	S_CMPK_LE_I32	SCC = (S0.i32 <= signext(SIMM16[15:0])).
9	S_CMPK_EQ_U32	SCC = (S0.u32 == SIMM16[15:0]).
10	S_CMPK_LG_U32	SCC = (S0.u32 != SIMM16[15:0]).
11	S_CMPK_GT_U32	SCC = (S0.u32 > SIMM16[15:0]).
12	S_CMPK_GE_U32	SCC = (S0.u32 >= SIMM16[15:0]).
13	S_CMPK_LT_U32	SCC = (S0.u32 < SIMM16[15:0]).
14	S_CMPK_LE_U32	SCC = (S0.u32 <= SIMM16[15:0]).
15	S_ADDK_I32	Add a 16-bit signed constant to the destination. int32 tmp = D.i32; // save value so we can check sign bits for overflow later. D.i32 = D.i32 + signext(SIMM16[15:0]); SCC = (tmp[31] == SIMM16[15] && tmp[31] != D.i32[31]). // signed overflow.
16	S_MULK_I32	Multiply a 16-bit signed constant with the destination. D.i32 = D.i32 * signext(SIMM16[15:0]).
18	S_GETREG_B32	Read some or all of a hardware register into the LSBs of D. SIMM16 = {size[4:0], offset[4:0], hwRegId[5:0]}; offset is 0..31, size is 1..32. uint32 offset = SIMM16[10:6]; uint32 size = SIMM16[15:11]; uint32 id = SIMM16[5:0]; D.u32 = hardware_reg[id][offset+size-1:offset].
19	S_SETREG_B32	Write some or all of the LSBs of S0 into a hardware register. SIMM16 = {size[4:0], offset[4:0], hwRegId[5:0]}; offset is 0..31, size is 1..32. hardware-reg = S0.u.
21	S_SETREG_IMM32_B32	Write some or all of the LSBs of IMM32 into a hardware register; this instruction requires a 32-bit literal constant. SIMM16 = {size[4:0], offset[4:0], hwRegId[5:0]}; offset is 0..31, size is 1..32. hardware-reg = LITERAL.
22	S_CALL_B64	Implements a short call, where the return address (the next instruction after the S_CALL_B64) is saved to D.  Long calls should consider S_SWAPPC_B64 instead. D.u64 = PC + 4; PC = PC + signext(SIMM16 * 4) + 4.
23	S_WAITCNT_VSCNT	Wait for the counts of outstanding vector store events -- vector memory stores and atomics that DO NOT return data -- to be at or below the specified level.  This counter is not used in 'all-in-order' mode. Waits for the following condition to hold before continuing: vscnt <= S0.u[5:0] + S1.u[5:0]. // Comparison is 6 bits, no clamping is applied for add overflow To wait on a literal constant only, write 'null' for the GPR argument. See also S_WAITCNT.
24	S_WAITCNT_VMCNT	Wait for the counts of outstanding vector memory events -- everything except for memory stores and atomics-without-return -- to be at or below the specified level.  When in 'all-in-order' mode, wait for all vector memory events. Waits for the following condition to hold before continuing: vmcnt <= S0.u[5:0] + S1.u[5:0]. // Comparison is 6 bits, no clamping is applied for add overflow To wait on a literal constant only, write 'null' for the GPR argument or use S_WAITCNT. See also S_WAITCNT.
25	S_WAITCNT_EXPCNT	Waits for the following condition to hold before continuing: expcnt <= S0.u[2:0] + S1.u[2:0]. // Comparison is 3 bits, no clamping is applied for add overflow To wait on a literal constant only, write 'null' for the GPR argument or use S_WAITCNT. See also S_WAITCNT.
26	S_WAITCNT_LGKMCNT	Waits for the following condition to hold before continuing: lgkmcnt <= S0.u[5:0] + S1.u[5:0]. // Comparison is 6 bits, no clamping is applied for add overflow To wait on a literal constant only, write 'null' for the GPR argument or use S_WAITCNT. See also S_WAITCNT.
27	S_SUBVECTOR_LOOP_BEGIN	Begin execution of a subvector block of code. See also S_SUBVECTOR_LOOP_END. if(EXEC[63:0] == 0) // no passes, skip entire loop jump LABEL elif(EXEC_LO == 0) // execute high pass only D0 = EXEC_LO else // execute low pass first, either running both passes or running low pass only D0 = EXEC_HI EXEC_HI = 0 endif. Example: s_subvector_loop_begin s0, SKIP_ALL LOOP_START: // instructions // ... LOOP_END: s_subvector_loop_end s0, LOOP_START SKIP_ALL: This opcode is intended to be used in conjunction with S_SUBVECTOR_LOOP_END but there is no dedicated subvector state and internally it is equivalent to an S_CBRANCH with extra math. This opcode has well-defined semantics in wave32 mode but the author of this document is not aware of any practical wave32 programming scenario where it would make sense to use this opcode.
28	S_SUBVECTOR_LOOP_END	End execution of a subvector block of code. See also S_SUBVECTOR_LOOP_START. if(EXEC_HI != 0) EXEC_LO = D0 elif(S0 == 0) // done: executed low pass and skip high pass nop else // execute second pass of two-pass mode EXEC_HI = D0 D0 = EXEC_LO EXEC_LO = 0 jump LABEL endif. This opcode is intended to be used in conjunction with S_SUBVECTOR_LOOP_BEGIN but there is no dedicated subvector state and internally it is equivalent to an S_CBRANCH with extra math. This opcode has well-defined semantics in wave32 mode but the author of this document is not aware of any practical wave32 programming scenario where it would make sense to use this opcode.

# SOP1 Instructions
3	S_MOV_B32	Move data to an SGPR. D.u = S0.u.
4	S_MOV_B64	Move data to an SGPR. D.u64 = S0.u64.
5	S_CMOV_B32	Conditionally move data to an SGPR when scalar condition code is true. if(SCC) then D.u = S0.u; endif.
6	S_CMOV_B64	Conditionally move data to an SGPR when scalar condition code is true. if(SCC) then D.u64 = S0.u64; endif.
7	S_NOT_B32	Bitwise negation. D = ~S0; SCC = (D != 0).
8	S_NOT_B64	Bitwise negation. D = ~S0; SCC = (D != 0).
9	S_WQM_B32	Computes whole quad mode for an active/valid mask.  If any pixel in a quad is active, all pixels of the quad are marked active. for i in 0 ... opcode_size_in_bits - 1 do D[i] = (S0[(i & ~3):(i | 3)] != 0); endfor; SCC = (D != 0).
10	S_WQM_B64	Computes whole quad mode for an active/valid mask.  If any pixel in a quad is active, all pixels of the quad are marked active. for i in 0 ... opcode_size_in_bits - 1 do D[i] = (S0[(i & ~3):(i | 3)] != 0); endfor; SCC = (D != 0).
11	S_BREV_B32	Reverse bits. D.u[31:0] = S0.u[0:31].
12	S_BREV_B64	Reverse bits. D.u64[63:0] = S0.u64[0:63].
13	S_BCNT0_I32_B32	Count number of bits that are zero. D = 0; for i in 0 ... opcode_size_in_bits - 1 do D += (S0[i] == 0 ? 1 : 0) endfor; SCC = (D != 0). Functional examples: S_BCNT0_I32_B32(0x00000000) => 32 S_BCNT0_I32_B32(0xcccccccc) => 16 S_BCNT0_I32_B32(0xffffffff) => 0
14	S_BCNT0_I32_B64	Count number of bits that are zero. D = 0; for i in 0 ... opcode_size_in_bits - 1 do D += (S0[i] == 0 ? 1 : 0) endfor; SCC = (D != 0). Functional examples: S_BCNT0_I32_B32(0x00000000) => 32 S_BCNT0_I32_B32(0xcccccccc) => 16 S_BCNT0_I32_B32(0xffffffff) => 0
15	S_BCNT1_I32_B32	Count number of bits that are one. D = 0; for i in 0 ... opcode_size_in_bits - 1 do D += (S0[i] == 1 ? 1 : 0) endfor; SCC = (D != 0). Functional examples: S_BCNT1_I32_B32(0x00000000) => 0 S_BCNT1_I32_B32(0xcccccccc) => 16 S_BCNT1_I32_B32(0xffffffff) => 32
16	S_BCNT1_I32_B64	Count number of bits that are one. D = 0; for i in 0 ... opcode_size_in_bits - 1 do D += (S0[i] == 1 ? 1 : 0) endfor; SCC = (D != 0). Functional examples: S_BCNT1_I32_B32(0x00000000) => 0 S_BCNT1_I32_B32(0xcccccccc) => 16 S_BCNT1_I32_B32(0xffffffff) => 32
17	S_FF0_I32_B32	Returns the bit position of the first zero from the LSB (least significant bit), or -1 if there are no zeros. D.i = -1; // Set if no zeros are found for i in 0 ... opcode_size_in_bits - 1 do // Search from LSB if S0[i] == 0 then D.i = i; break for; endif; endfor. Functional examples: S_FF0_I32_B32(0xaaaaaaaa) => 0 S_FF0_I32_B32(0x55555555) => 1 S_FF0_I32_B32(0x00000000) => 0 S_FF0_I32_B32(0xffffffff) => 0xffffffff S_FF0_I32_B32(0xfffeffff) => 16
18	S_FF0_I32_B64	Returns the bit position of the first zero from the LSB (least significant bit), or -1 if there are no zeros. D.i = -1; // Set if no zeros are found for i in 0 ... opcode_size_in_bits - 1 do // Search from LSB if S0[i] == 0 then D.i = i; break for; endif; endfor. Functional examples: S_FF0_I32_B32(0xaaaaaaaa) => 0 S_FF0_I32_B32(0x55555555) => 1 S_FF0_I32_B32(0x00000000) => 0 S_FF0_I32_B32(0xffffffff) => 0xffffffff S_FF0_I32_B32(0xfffeffff) => 16
19	S_FF1_I32_B32	Returns the bit position of the first one from the LSB (least significant bit), or -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... opcode_size_in_bits - 1 do // Search from LSB if S0[i] == 1 then D.i = i; break for; endif; endfor. Functional examples: S_FF1_I32_B32(0xaaaaaaaa) => 1 S_FF1_I32_B32(0x55555555) => 0 S_FF1_I32_B32(0x00000000) => 0xffffffff S_FF1_I32_B32(0xffffffff) => 0 S_FF1_I32_B32(0x00010000) => 16
20	S_FF1_I32_B64	Returns the bit position of the first one from the LSB (least significant bit), or -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... opcode_size_in_bits - 1 do // Search from LSB if S0[i] == 1 then D.i = i; break for; endif; endfor. Functional examples: S_FF1_I32_B32(0xaaaaaaaa) => 1 S_FF1_I32_B32(0x55555555) => 0 S_FF1_I32_B32(0x00000000) => 0xffffffff S_FF1_I32_B32(0xffffffff) => 0 S_FF1_I32_B32(0x00010000) => 16
21	S_FLBIT_I32_B32	Counts how many zeros before the first one starting from the MSB (most significant bit).  Returns -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... opcode_size_in_bits - 1 do // Note: search is from the MSB if S0[opcode_size_in_bits - 1 - i] == 1 then D.i = i; break for; endif; endfor. Functional examples: S_FLBIT_I32_B32(0x00000000) => 0xffffffff S_FLBIT_I32_B32(0x0000cccc) => 16 S_FLBIT_I32_B32(0xffff3333) => 0 S_FLBIT_I32_B32(0x7fffffff) => 1 S_FLBIT_I32_B32(0x80000000) => 0 S_FLBIT_I32_B32(0xffffffff) => 0
22	S_FLBIT_I32_B64	Counts how many zeros before the first one starting from the MSB (most significant bit).  Returns -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... opcode_size_in_bits - 1 do // Note: search is from the MSB if S0[opcode_size_in_bits - 1 - i] == 1 then D.i = i; break for; endif; endfor. Functional examples: S_FLBIT_I32_B32(0x00000000) => 0xffffffff S_FLBIT_I32_B32(0x0000cccc) => 16 S_FLBIT_I32_B32(0xffff3333) => 0 S_FLBIT_I32_B32(0x7fffffff) => 1 S_FLBIT_I32_B32(0x80000000) => 0 S_FLBIT_I32_B32(0xffffffff) => 0
23	S_FLBIT_I32	Counts how many bits in a row (from MSB to LSB) are the same as the sign bit.  Returns -1 if all bits are the same. D.i = -1; // Set if all bits are the same for i in 1 ... opcode_size_in_bits - 1 do // Note: search is from the MSB if S0[opcode_size_in_bits - 1 - i] != S0[opcode_size_in_bits - 1] then D.i = i; break for; endif; endfor. Functional examples: S_FLBIT_I32(0x00000000) => 0xffffffff S_FLBIT_I32(0x0000cccc) => 16 S_FLBIT_I32(0xffff3333) => 16 S_FLBIT_I32(0x7fffffff) => 1 S_FLBIT_I32(0x80000000) => 1 S_FLBIT_I32(0xffffffff) => 0xffffffff
24	S_FLBIT_I32_I64	Counts how many bits in a row (from MSB to LSB) are the same as the sign bit.  Returns -1 if all bits are the same. D.i = -1; // Set if all bits are the same for i in 1 ... opcode_size_in_bits - 1 do // Note: search is from the MSB if S0[opcode_size_in_bits - 1 - i] != S0[opcode_size_in_bits - 1] then D.i = i; break for; endif; endfor. Functional examples: S_FLBIT_I32(0x00000000) => 0xffffffff S_FLBIT_I32(0x0000cccc) => 16 S_FLBIT_I32(0xffff3333) => 16 S_FLBIT_I32(0x7fffffff) => 1 S_FLBIT_I32(0x80000000) => 1 S_FLBIT_I32(0xffffffff) => 0xffffffff
25	S_SEXT_I32_I8	Sign extension of a signed byte. D.i = signext(S0.i[7:0]).
26	S_SEXT_I32_I16	Sign extension of a signed short. D.i = signext(S0.i[15:0]).
27	S_BITSET0_B32	Set a specific bit to zero. D.u[S0.u[4:0]] = 0.
28	S_BITSET0_B64	Set a specific bit to zero. D.u64[S0.u[5:0]] = 0.
29	S_BITSET1_B32	Set a specific bit to one. D.u[S0.u[4:0]] = 1.
30	S_BITSET1_B64	Set a specific bit to one. D.u64[S0.u[5:0]] = 1.
31	S_GETPC_B64	Save current program location.  Destination receives the byte address of the next instruction. D.u64 = PC + 4.
32	S_SETPC_B64	Jump to a new location.  S0.u64 is a byte address of the instruction to jump to. PC = S0.u64.
33	S_SWAPPC_B64	Save current program location and jump to a new location. S0.u64 is a byte address of the instruction to jump to. Destination receives the byte address of the instruction immediately following the SWAPPC instruction. D.u64 = PC + 4; PC = S0.u64.
34	S_RFE_B64	Return from exception handler and continue.  This instruction may only be used within a trap handler. PRIV = 0; PC = S0.u64.
36	S_AND_SAVEEXEC_B64	Bitwise AND with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = S0.u64 & EXEC; SCC = (EXEC != 0).
37	S_OR_SAVEEXEC_B64	Bitwise OR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = S0.u64 | EXEC; SCC = (EXEC != 0).
38	S_XOR_SAVEEXEC_B64	Bitwise XOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = S0.u64 ^ EXEC; SCC = (EXEC != 0).
39	S_ANDN2_SAVEEXEC_B64	Bitwise ANDN2 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = S0.u64 & ~EXEC; SCC = (EXEC != 0).
40	S_ORN2_SAVEEXEC_B64	Bitwise ORN2 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = S0.u64 | ~EXEC; SCC = (EXEC != 0).
41	S_NAND_SAVEEXEC_B64	Bitwise NAND with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = ~(S0.u64 & EXEC); SCC = (EXEC != 0).
42	S_NOR_SAVEEXEC_B64	Bitwise NOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = ~(S0.u64 | EXEC); SCC = (EXEC != 0).
43	S_XNOR_SAVEEXEC_B64	Bitwise XNOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = ~(S0.u64 ^ EXEC); SCC = (EXEC != 0).
44	S_QUADMASK_B32	Reduce a pixel mask to a quad mask.  To perform the inverse operation see S_BITREPLICATE_B64_B32. D = 0; for i in 0 ... (opcode_size_in_bits / 4) - 1 do D[i] = (S0[i * 4 + 3:i * 4] != 0); endfor; SCC = (D != 0).
45	S_QUADMASK_B64	Reduce a pixel mask to a quad mask.  To perform the inverse operation see S_BITREPLICATE_B64_B32. D = 0; for i in 0 ... (opcode_size_in_bits / 4) - 1 do D[i] = (S0[i * 4 + 3:i * 4] != 0); endfor; SCC = (D != 0).
46	S_MOVRELS_B32	Move from a relative source address. SGPR[D.addr].u32 = SGPR[S0.addr+M0[31:0]].u32 Example: The following instruction sequence will perform a move s5 <== s17: s_mov_b32 m0, 10 s_movrels_b32 s5, s7
47	S_MOVRELS_B64	Move from a relative source address.  The index in M0.u must be even for this operation. SGPR[D.addr].u64 = SGPR[S0.addr+M0[31:0]].u64
48	S_MOVRELD_B32	Move to a relative destination address. SGPR[D.addr+M0[31:0]].u32 = SGPR[S0.addr].u32 Example: The following instruction sequence will perform a move s15 <== s7: s_mov_b32 m0, 10 s_movreld_b32 s5, s7
49	S_MOVRELD_B64	Move to a relative destination address.  The index in M0.u must be even for this operation. SGPR[D.addr+M0[31:0]].u64 = SGPR[S0.addr].u64
52	S_ABS_I32	Integer absolute value. D.i = (S.i < 0 ? -S.i : S.i); SCC = (D.i != 0). Functional examples: S_ABS_I32(0x00000001) => 0x00000001 S_ABS_I32(0x7fffffff) => 0x7fffffff S_ABS_I32(0x80000000) => 0x80000000// Note this is negative! S_ABS_I32(0x80000001) => 0x7fffffff S_ABS_I32(0x80000002) => 0x7ffffffe S_ABS_I32(0xffffffff) => 0x00000001
55	S_ANDN1_SAVEEXEC_B64	Bitwise ANDN1 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = ~S0.u64 & EXEC; SCC = (EXEC != 0).
56	S_ORN1_SAVEEXEC_B64	Bitwise ORN1 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u64 = EXEC; EXEC = ~S0.u64 | EXEC; SCC = (EXEC != 0).
57	S_ANDN1_WREXEC_B64	Bitwise ANDN1 with EXEC mask.  Unlike the SAVEEXEC series of opcodes, the value written to destination SGPRs is the result of the bitwise-op result.  EXEC and the destination SGPRs will have the same value at the end of this instruction.  This instruction is intended to accelerate waterfalling. EXEC = ~S0.u64 & EXEC; D.u64 = EXEC; SCC = (EXEC != 0).
58	S_ANDN2_WREXEC_B64	Bitwise ANDN2 with EXEC mask.  Unlike the SAVEEXEC series of opcodes, the value written to destination SGPRs is the result of the bitwise-op result.  EXEC and the destination SGPRs will have the same value at the end of this instruction.  This instruction is intended to accelerate waterfalling. EXEC = S0.u64 & ~EXEC; D.u64 = EXEC; SCC = (EXEC != 0). In particular, the following sequence of waterfall code is optimized by using a WREXEC instead of two separate scalar ops: // V0 holds the index value per lane // save exec mask for restore at the end s_mov_b64 s2, exec // exec mask of remaining (unprocessed) threads s_mov_b64 s4, exec loop: // get the index value for the first active lane v_readfirstlane_b32  s0, v0 // find all other lanes with same index value v_cmpx_eq s0, v0 <OP>// do the operation using the current EXEC mask. S0 holds the index. // mask out thread that was just executed // s_andn2_b64  s4, s4, exec // s_mov_b64exec, s4 s_andn2_wrexec_b64 s4, s4// replaces above 2 ops // repeat until EXEC==0 s_cbranch_scc1  loop s_mov_b64exec, s2
59	S_BITREPLICATE_B64_B32	Replicate the low 32 bits of S0 by 'doubling' each bit. for i in 0 ... 31 do D.u64[i * 2 + 0] = S0.u32[i] D.u64[i * 2 + 1] = S0.u32[i] endfor. This opcode can be used to convert a quad mask into a pixel mask; given quad mask in s0, the following sequence will produce a pixel mask in s2: s_bitreplicate_b64 s2, s0 s_bitreplicate_b64 s2, s2 To perform the inverse operation see S_QUADMASK_B64.
60	S_AND_SAVEEXEC_B32	Bitwise AND with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = S0.u32 & EXEC_LO; SCC = (EXEC_LO != 0).
61	S_OR_SAVEEXEC_B32	Bitwise OR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = S0.u32 | EXEC_LO; SCC = (EXEC_LO != 0).
62	S_XOR_SAVEEXEC_B32	Bitwise XOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = S0.u32 ^ EXEC_LO; SCC = (EXEC_LO != 0).
63	S_ANDN2_SAVEEXEC_B32	Bitwise ANDN2 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = S0.u32 & ~EXEC_LO; SCC = (EXEC_LO != 0).
64	S_ORN2_SAVEEXEC_B32	Bitwise ORN2 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = S0.u32 | ~EXEC_LO; SCC = (EXEC_LO != 0).
65	S_NAND_SAVEEXEC_B32	Bitwise NAND with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = ~(S0.u32 & EXEC_LO); SCC = (EXEC_LO != 0).
66	S_NOR_SAVEEXEC_B32	Bitwise NOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = ~(S0.u32 | EXEC_LO); SCC = (EXEC_LO != 0).
67	S_XNOR_SAVEEXEC_B32	Bitwise XNOR with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = ~(S0.u32 ^ EXEC_LO); SCC = (EXEC_LO != 0).
68	S_ANDN1_SAVEEXEC_B32	Bitwise ANDN1 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = ~S0.u32 & EXEC_LO; SCC = (EXEC_LO != 0).
69	S_ORN1_SAVEEXEC_B32	Bitwise ORN1 with EXEC mask.  The original EXEC mask is saved to the destination SGPRs before the bitwise operation is performed. D.u32 = EXEC_LO; EXEC_LO = ~S0.u32 | EXEC_LO; SCC = (EXEC_LO != 0).
70	S_ANDN1_WREXEC_B32	Bitwise ANDN1 with EXEC mask.  Unlike the SAVEEXEC series of opcodes, the value written to destination SGPRs is the result of the bitwise-op result.  EXEC and the destination SGPRs will have the same value at the end of this instruction.  This instruction is intended to accelerate waterfalling. EXEC_LO = ~S0.u32 & EXEC_LO; D.u32 = EXEC_LO; SCC = (EXEC_LO != 0).
71	S_ANDN2_WREXEC_B32	Bitwise ANDN2 with EXEC mask.  Unlike the SAVEEXEC series of opcodes, the value written to destination SGPRs is the result of the bitwise-op result.  EXEC and the destination SGPRs will have the same value at the end of this instruction.  This instruction is intended to accelerate waterfalling.  See S_ANDN2_WREXEC_B64 for example code. EXEC_LO = S0.u32 & ~EXEC_LO; D.u32 = EXEC_LO; SCC = (EXEC_LO != 0).
73	S_MOVRELSD_2_B32	Move from a relative source address to a relative destination address, with different offsets. SGPR[D.addr+M0[25:16]].u32 = SGPR[S0.addr+M0[9:0]].u32 Example: The following instruction sequence will perform a move s25 <== s17: s_mov_b32 m0, ((20 << 16) | 10) s_movrelsd_2_b32 s5, s7

# SOPC Instructions
0	S_CMP_EQ_I32	Compare two integers for equality.  Note that S_CMP_EQ_I32 and S_CMP_EQ_U32 are identical opcodes, but both are provided for symmetry. SCC = (S0 == S1).
1	S_CMP_LG_I32	Compare two integers for inequality.  Note that S_CMP_LG_I32 and S_CMP_LG_U32 are identical opcodes, but both are provided for symmetry. SCC = (S0 != S1).
2	S_CMP_GT_I32	SCC = (S0.i > S1.i).
3	S_CMP_GE_I32	SCC = (S0.i >= S1.i).
4	S_CMP_LT_I32	SCC = (S0.i < S1.i).
5	S_CMP_LE_I32	SCC = (S0.i <= S1.i).
6	S_CMP_EQ_U32	Compare two integers for equality.  Note that S_CMP_EQ_I32 and S_CMP_EQ_U32 are identical opcodes, but both are provided for symmetry. SCC = (S0 == S1).
7	S_CMP_LG_U32	Compare two integers for inequality.  Note that S_CMP_LG_I32 and S_CMP_LG_U32 are identical opcodes, but both are provided for symmetry. SCC = (S0 != S1).
8	S_CMP_GT_U32	SCC = (S0.u > S1.u).
9	S_CMP_GE_U32	SCC = (S0.u >= S1.u).
10	S_CMP_LT_U32	SCC = (S0.u < S1.u).
11	S_CMP_LE_U32	SCC = (S0.u <= S1.u).
12	S_BITCMP0_B32	SCC = (S0.u[S1.u[4:0]] == 0).
13	S_BITCMP1_B32	SCC = (S0.u[S1.u[4:0]] == 1).
14	S_BITCMP0_B64	SCC = (S0.u64[S1.u[5:0]] == 0).
15	S_BITCMP1_B64	SCC = (S0.u64[S1.u[5:0]] == 1).
18	S_CMP_EQ_U64	SCC = (S0.i64 == S1.i64).
19	S_CMP_LG_U64	SCC = (S0.i64 != S1.i64).

# SOPP Instructions
0	S_NOP	Do nothing. Repeat NOP 1..16 times based on SIMM16[3:0] -- 0x0 = 1 time, 0xf = 16 times. Examples: s_nop 0        // Wait 1 cycle. s_nop 0xf      // Wait 16 cycles.
1	S_ENDPGM	End of program; terminate wavefront.  The hardware implicitly executes S_WAITCNT 0 and S_WAITCNT_VSCNT 0 before executing this instruction.  See S_ENDPGM_SAVED for the context-switch version of this instruction and S_ENDPGM_ORDERED_PS_DONE for the POPS critical region version of this instruction.
2	S_BRANCH	Perform an unconditional short jump.  For a long jump, use S_SETPC_B64. PC = PC + signext(SIMM16 * 4) + 4. // short jump. Examples: s_branch label   // Set SIMM16 = +4 = 0x0004 s_nop 0   // 4 bytes label: s_nop 0   // 4 bytes s_branch label   // Set SIMM16 = -8 = 0xfff8
3	S_WAKEUP	Allow a wave to 'ping' all the other waves in its threadgroup to force them to wake up early from an S_SLEEP instruction. The ping is ignored if the waves are not sleeping.  This allows for efficient polling on a memory location. The waves which are polling can sit in a long S_SLEEP between memory reads, but the wave which writes the value can tell them all to wake up early now that the data is available. This is useful for fBarrier implementations (speedup).  This method is also safe from races because if any wave misses the ping, everything still works fine (waves which missed it just complete their S_SLEEP). If the wave executing S_WAKEUP is in a threadgroup (in_tg set), then it will wake up all waves associated with the same threadgroup ID. Otherwise, S_WAKEUP is treated as an S_NOP.
4	S_CBRANCH_SCC0	Perform a conditional short jump when SCC is zero. if(SCC == 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
5	S_CBRANCH_SCC1	Perform a conditional short jump when SCC is one. if(SCC == 1) then PC = PC + signext(SIMM16 * 4) + 4; endif.
6	S_CBRANCH_VCCZ	Perform a conditional short jump when VCC is zero. if(VCC == 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
7	S_CBRANCH_VCCNZ	Perform a conditional short jump when VCC is nonzero. if(VCC != 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
8	S_CBRANCH_EXECZ	Perform a conditional short jump when EXEC is zero. if(EXEC == 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
9	S_CBRANCH_EXECNZ	Perform a conditional short jump when EXEC is nonzero. if(EXEC != 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
10	S_BARRIER	Synchronize waves within a threadgroup.  If not all waves of the threadgroup have been created yet, waits for entire group before proceeding.  If some waves in the threadgroup have already terminated, this waits on only the surviving waves.  Barriers are legal inside trap handlers.
11	S_SETKILL	Set KILL bit to value of SIMM16[0].  Used primarily for debugging kill wave host command behavior.
12	S_WAITCNT	Wait for the counts of outstanding lds, vector-memory and export/vmem-write-data to be at or below the specified levels. Waits for all of the following conditions to hold before continuing: vmcnt <= {SIMM16[15:14], SIMM16[3:0]} expcnt <= SIMM16[6:4] lgkmcnt <= SIMM16[13:8] NOTE: VMCNT only counts vector memory loads, image sample instructions, and vector memory atomics that return data.  Contrast with the VSCNT counter. See also S_WAITCNT_VSCNT.
13	S_SETHALT	S_SETHALT can set/clear the HALT or FATAL_HALT status bits.  The particular status bit is chosen by halt type control as indicated in SIMM16[2]; 0 = HALT bit select; 1 = FATAL_HALT bit select. When halt type control is set to 0 = HALT bit select: Set HALT bit to value of SIMM16[0]; 1 = halt, 0 = clear HALT bit. The halt flag is ignored while PRIV == 1 (inside trap handlers) but the shader will halt after the handler returns if HALT is still set at that time. When halt type control is set to 1 = FATAL HALT bit select: Set FATAL_HALT bit to value of SIMM16[0]; 1 = fatal_halt, 0 = clear FATAL_HALT bit.  Setting the fatal_halt flag halts the shader in or outside of the trap handlers.
14	S_SLEEP	Cause a wave to sleep for (64*(SIMM16[6:0]-1) .. 64*SIMM16[6:0]) clocks.  The exact amount of delay is approximate.  Compare with S_NOP.  When SIMM16[6:0] is zero then no sleep occurs. Examples: s_sleep 0      // Wait for 0 clocks. s_sleep 1      // Wait for 1-64 clocks. s_sleep 2      // Wait for 65-128 clocks.
15	S_SETPRIO	User settable wave priority is set to SIMM16[1:0]. 0 = lowest, 3 = highest.  The overall wave priority is {SPIPrio[1:0] + UserPrio[1:0], WaveAge[3:0]}.
16	S_SENDMSG	Send a message upstream to VGT or the interrupt handler. SIMM16[9:0] contains the message type.
17	S_SENDMSGHALT	Send a message and then HALT the wavefront; see S_SENDMSG for details.
18	S_TRAP	Enter the trap handler.  This instruction may be generated internally as well in response to a host trap (HT = 1) or an exception.  TrapID 0 is reserved for hardware use and should not be used in a shader-generated trap. TrapID = SIMM16[7:0]; Wait for all instructions to complete; {TTMP1, TTMP0} = {1'h0, PCRewind[5:0], HT[0], TrapID[7:0], PC[47:0]}; PC = TBA; // trap base address PRIV = 1.
19	S_ICACHE_INV	Invalidate entire L0 instruction cache. The hardware invalidates the instruction buffer, so no S_NOP instructions are required after S_ICACHE_INV.
20	S_INCPERFLEVEL	Increment performance counter specified in SIMM16[3:0] by 1.
21	S_DECPERFLEVEL	Decrement performance counter specified in SIMM16[3:0] by 1.
22	S_TTRACEDATA	Send M0 as user data to the thread trace stream.
23	S_CBRANCH_CDBGSYS	 Perform a conditional short jump when the system debug flag is set. if(conditional_debug_system != 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
24	S_CBRANCH_CDBGUSER	 Perform a conditional short jump when the user debug flag is set. if(conditional_debug_user != 0) then PC = PC + signext(SIMM16 * 4) + 4; endif.
25	S_CBRANCH_CDBGSYS_OR_USER	 Perform a conditional short jump when either the system or the user debug flag are set. if(conditional_debug_system || conditional_debug_user) then PC = PC + signext(SIMM16 * 4) + 4; endif.
26	S_CBRANCH_CDBGSYS_AND_USER	 Perform a conditional short jump when both the system and the user debug flag are set. if(conditional_debug_system && conditional_debug_user) then PC = PC + signext(SIMM16 * 4) + 4; endif.
27	S_ENDPGM_SAVED	End of program; signal that a wave has been saved by the context-switch trap handler and terminate wavefront.  The hardware implicitly executes S_WAITCNT 0 and S_WAITCNT_VSCNT 0 before executing this instruction.  See S_ENDPGM for additional variants.
30	S_ENDPGM_ORDERED_PS_DONE	 End of program; signal that a wave has exited its POPS critical section and terminate wavefront.  The hardware implicitly executes S_WAITCNT 0 and S_WAITCNT_VSCNT 0 before executing this instruction. This instruction is an optimization that combines S_SENDMSG(MSG_ORDERED_PS_DONE) and S_ENDPGM; there may be cases where you still need to send the message separately, in which case the shader must end with a regular S_ENDPGM instruction. See S_ENDPGM for additional variants.
31	S_CODE_END	Generate an illegal instruction interrupt. This instruction should NEVER appear in typical shader code.  It is used to pad the end of a shader program to make it easier for analysis programs to locate the end of a shader program buffer. Use of this opcode in an embedded shader block may cause analysis tools to fail. To unambiguously mark the end of a shader buffer, this instruction must be specified five times in a row (total of 20 bytes) and analysis tools must ensure the opcode occurs at least five times to be certain they are at the end of the buffer. This is because the bit pattern generated by this opcode could incidentally appear in a valid instruction's second dword, literal constant or as part of a multi-DWORD image instruction. In short: do not embed this opcode in the middle of a valid shader program. DO use this opcode 5 times at the end of a shader program to clearly mark the end of the program. Example: ... s_endpgm       // last real instruction in shader buffer s_code_end     // 1 s_code_end     // 2 s_code_end     // 3 s_code_end     // 4 s_code_end     // done!
32	S_INST_PREFETCH	Change instruction prefetch mode. SIMM16[1:0] specifies the prefetch mode to switch to. Defined prefetch modes are: 0: reserved 1: SQ_PREFETCH_1_LINE -- prefetch 1 line 2: SQ_PREFETCH_2_LINES -- prefetch 2 lines 3: SQ_PREFETCH_3_LINES -- prefetch 3 lines SIMM16[15:2] must be set to zero.
33	S_CLAUSE	Mark the beginning of a clause. The next instruction determines the clause type, which may be one of the following types. Texture, Buffer, Global, Scratch (clause may not mix atomics, loads & stores) Flat (loads, stores and atomics may not be combined in a clause) LDS SMEM VALU Halting and killing a wave will break the clause. The clause length is: (SIMM16[5:0] + 1), and clauses must be 2 instructions or longer and no more than 63 instructions. SIMM16[11:8] determines the number of instructions per clause break, in the range 0..15. If SIMM16[11:8] == 0 then there are no clause breaks. The following instruction types cannot appear in a clause: SALU Export Branch Message GDS
35	S_WAITCNT_DEPCTR	Bit mask of which dependency counters to wait to be zero, intended for debug and bug-workarounds. Waits for all of the following conditions to hold before continuing: va_vdst <= SIMM16[15:12] || SIMM16[15:12] == 0xf va_sdst <= SIMM16[11:9] || SIMM16[11:9] == 0x7 va_ssrc == 0 || SIMM16[8] == 1 hold_cnt == 0 || SIMM16[7] == 1 vm_vsrc <= SIMM16[4:2] || SIMM16[4:2] == 0x7 va_vcc == 0 || SIMM16[1] == 1 sa_sdst == 0 || SIMM16[0] == 1 Some wait values are smaller than the counters: the max wait value means don't wait on this counter.  For example, VM_VSRC is 4 bits, but the wait field for VM_VSRC is only 3 bits.  The value 7 means don't wait on VM_VSRC, 6 means wait for VM_VSRC <= 6, etc.  The wait value for VA_VCC is just 1 bit even though the counter is 3 bits: 0 = wait for va_vcc==0, 1 = don't wait on va_vcc.
36	S_ROUND_MODE	Set floating point round mode using an immediate constant. Avoids wait state penalty that would be imposed by S_SETREG.
37	S_DENORM_MODE	Set floating point denormal mode using an immediate constant. Avoids wait state penalty that would be imposed by S_SETREG.
40	S_TTRACEDATA_IMM	Send SIMM16[7:0] as user data to the thread trace stream. 2=emit,	 	vertices, M0[22:12] = number of primitives.

# SMEM Instructions
0	S_LOAD_DWORD	Read 1 dword from scalar data cache. If the offset is specified as an SGPR, the SGPR contains an UNSIGNED BYTE offset (the 2 LSBs are ignored). If the offset is specified as an immediate 21-bit constant, the constant is a SIGNED BYTE offset.
1	S_LOAD_DWORDX2	Read 2 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
2	S_LOAD_DWORDX4	Read 4 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
3	S_LOAD_DWORDX8	Read 8 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
4	S_LOAD_DWORDX16	Read 16 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
8	S_BUFFER_LOAD_DWORD	Read 1 dword from scalar data cache. See S_LOAD_DWORD for details on the offset input.
9	S_BUFFER_LOAD_DWORDX2	Read 2 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
10	S_BUFFER_LOAD_DWORDX4	Read 4 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
11	S_BUFFER_LOAD_DWORDX8	Read 8 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
12	S_BUFFER_LOAD_DWORDX16	Read 16 dwords from scalar data cache. See S_LOAD_DWORD for details on the offset input.
31	S_GL1_INV	Invalidate the GL1 cache only.
32	S_DCACHE_INV	Invalidate the scalar data L0 cache.
36	S_MEMTIME	Return current 64-bit timestamp.
37	S_MEMREALTIME	Return current 64-bit RTC.
38	S_ATC_PROBE	Probe or prefetch an address into the SQC data cache.
39	S_ATC_PROBE_BUFFER	Probe or prefetch an address into the SQC data cache.

# VOP2 Instructions
1	V_CNDMASK_B32	Conditional mask on each thread. In VOP3 the VCC source may be a scalar GPR specified in S2.u. Floating-point modifiers are valid for this instruction if S0.u and S1.u are 32-bit floating point values. This instruction is suitable for negating or taking the absolute value of a floating-point value. D.u32 = VCC ? S1.u32 : S0.u32.
2	V_DOT2C_F32_F16	Dot product of packed FP16 values, accumulate with destination. D.f32 = S0.f16[0] * S1.f16[0] + S0.f16[1] * S1.f16[1] + D.f32.
3	V_ADD_F32	Add two single-precision values.  0.5ULP precision, denormals are supported. D.f32 = S0.f32 + S1.f32.
4	V_SUB_F32	Subtract the second single-precision input from the first input. D.f32 = S0.f32 - S1.f32.
5	V_SUBREV_F32	Subtract the first single-precision input from the second input. D.f32 = S1.f32 - S0.f32.
6	V_FMAC_LEGACY_F32	Multiply two single-precision values and accumulate the result with the destination.  Follows DX9 rules where 0.0 times anything produces 0.0 (this is not IEEE compliant). D.f32 = S0.f32 * S1.f32 + S2.f32. // DX9 rules, 0.0 * x = 0.0
7	V_MUL_LEGACY_F32	Multiply two single-precision values.  Follows DX9 rules where 0.0 times anything produces 0.0 (this is not IEEE compliant). D.f32 = S0.f32 * S1.f32. // DX9 rules, 0.0*x = 0.0
8	V_MUL_F32	Multiply two single-precision values.  0.5ULP precision, denormals are supported. D.f32 = S0.f32 * S1.f32.
9	V_MUL_I32_I24	Multiply two signed 24-bit integers and store the result as a signed 32-bit integer.  This opcode is as efficient as basic single-precision opcodes since it utilizes the single-precision floating point multiplier.  See also V_MUL_HI_I32_I24. D.i32 = S0.i24 * S1.i24.
10	V_MUL_HI_I32_I24	Multiply two signed 24-bit integers and store the high 32 bits of the result as a signed 32-bit integer.  See also V_MUL_I32_I24. D.i32 = (S0.i24 * S1.i24)>>32;
11	V_MUL_U32_U24	Multiply two unsigned 24-bit integers and store the result as an unsigned 32-bit integer.  This opcode is as efficient as basic single-precision opcodes since it utilizes the single-precision floating point multiplier.  See also V_MUL_HI_U32_U24. D.u32 = S0.u24 * S1.u24.
12	V_MUL_HI_U32_U24	Multiply two unsigned 24-bit integers and store the high 32 bits of the result as an unsigned 32-bit integer.  See also V_MUL_U32_U24. D.u32 = (S0.u24 * S1.u24)>>32.
13	V_DOT4C_I32_I8	Dot product of packed byte values, accumulate with destination. D.i32 = S0.i8[0] * S1.i8[0] + S0.i8[1] * S1.i8[1] + S0.i8[2] * S1.i8[2] + S0.i8[3] * S1.i8[3] + D.i32.
15	V_MIN_F32	Compute the minimum of two single-precision floats. D.f32 = min(S0.f32,S1.f32); if (IEEE_MODE && S0.f == sNaN) D.f = Quiet(S0.f); else if (IEEE_MODE && S1.f == sNaN) D.f = Quiet(S1.f); else if (S0.f == NaN) D.f = S1.f; else if (S1.f == NaN) D.f = S0.f; else if (S0.f == +0.0 && S1.f == -0.0) D.f = S1.f; else if (S0.f == -0.0 && S1.f == +0.0) D.f = S0.f; else // Note: there's no IEEE special case here like there is for V_MAX_F32. D.f = (S0.f < S1.f ? S0.f : S1.f); endif.
16	V_MAX_F32	Compute the maximum of two single-precision floats. D.f32 = max(S0.f32,S1.f32); if (IEEE_MODE && S0.f == sNaN) D.f = Quiet(S0.f); else if (IEEE_MODE && S1.f == sNaN) D.f = Quiet(S1.f); else if (S0.f == NaN) D.f = S1.f; else if (S1.f == NaN) D.f = S0.f; else if (S0.f == +0.0 && S1.f == -0.0) D.f = S0.f; else if (S0.f == -0.0 && S1.f == +0.0) D.f = S1.f; else if (IEEE_MODE) D.f = (S0.f >= S1.f ? S0.f : S1.f); else D.f = (S0.f > S1.f ? S0.f : S1.f); endif.
17	V_MIN_I32	Compute the minimum of two signed integers. D.i32 = (S0.i32 < S1.i32 ? S0.i32 : S1.i32).
18	V_MAX_I32	Compute the maximum of two signed integers. D.i32 = (S0.i32 >= S1.i32 ? S0.i32 : S1.i32).
19	V_MIN_U32	Compute the minimum of two unsigned integers. D.u32 = (S0.u32 < S1.u32 ? S0.u32 : S1.u32).
20	V_MAX_U32	Compute the maximum of two unsigned integers. D.u32 = (S0.u32 >= S1.u32 ? S0.u32 : S1.u32).
22	V_LSHRREV_B32	Logical shift right with shift count in the first operand. D.u32 = S1.u32 >> S0[4:0].
24	V_ASHRREV_I32	Arithmetic shift right (preserve sign bit) with shift count in the first operand. D.i32 = S1.i32 >> S0[4:0].
26	V_LSHLREV_B32	Logical shift left with shift count in the first operand. D.u32 = S1.u32 << S0[4:0].
27	V_AND_B32	Bitwise AND.  Input and output modifiers not supported. D.u32 = S0.u32 & S1.u32.
28	V_OR_B32	Bitwise OR.  Input and output modifiers not supported. D.u32 = S0.u32 | S1.u32.
29	V_XOR_B32	Bitwise XOR.  Input and output modifiers not supported. D.u32 = S0.u32 ^ S1.u32.
30	V_XNOR_B32	Bitwise XNOR. Input and output modifiers not supported. D.u32 = ~(S0.u32 ^ S1.u32).
37	V_ADD_NC_U32	Add two unsigned integers. No carry-in or carry-out. D.u32 = S0.u32 + S1.u32.
38	V_SUB_NC_U32	Subtract the second unsigned integer from the first unsigned integer.  No carry-in or carry-out. D.u32 = S0.u32 - S1.u32.
39	V_SUBREV_NC_U32	Subtract the first unsigned integer from the second unsigned integer.  No carry-in or carry-out. D.u32 = S1.u32 - S0.u32.
40	V_ADD_CO_CI_U32	Add two unsigned integers and a carry-in from VCC.  Store the result and also save the carry-out to VCC.  In VOP3 the VCC destination may be an arbitrary SGPR-pair, and the VCC source comes from the SGPR-pair at S2.u. D.u32 = S0.u32 + S1.u32 + VCC; VCC = S0.u32 + S1.u32 + VCC >= 0x100000000ULL ? 1 : 0.
41	V_SUB_CO_CI_U32	Subtract the second unsigned integer from the first unsigned integer and then subtract a carry-in from VCC.  Store the result and also save the carry-out to VCC.  In VOP3 the VCC destination may be an arbitrary SGPR-pair, and the VCC source comes from the SGPR-pair at S2.u. D.u32 = S0.u32 - S1.u32 - VCC; VCC = S1.u32 + VCC > S0.u32 ? 1 : 0.
42	V_SUBREV_CO_CI_U3	Subtract the first unsigned integer from the second unsigned 2 integer and then subtract a carry-in from VCC.  Store the result and also save the carry-out to VCC.  In VOP3 the VCC destination may be an arbitrary SGPR-pair, and the VCC source comes from the SGPR-pair at S2.u. D.u32 = S1.u32 - S0.u32 - VCC; VCC = S1.u32 + VCC > S0.u ? 1 : 0.
43	V_FMAC_F32	Fused multiply-add of single-precision floats, accumulate with destination. D.f32 = S0.f32 * S1.f32 + D.f32. // Fused operation
44	V_FMAMK_F32	Multiply a single-precision float with a literal constant and add a second single-precision float using fused multiply-add. This opcode cannot use the VOP3 encoding and cannot use input/output modifiers. D.f32 = S0.f32 * K.f32 + S1.f32. // K is a 32-bit literal constant.
45	V_FMAAK_F32	Multiply two single-precision floats and add a literal constant using fused multiply-add.  This opcode cannot use the VOP3 encoding and cannot use input/output modifiers. D.f32 = S0.f32 * S1.f32 + K.f32. // K is a 32-bit literal constant.
47	V_CVT_PKRTZ_F16_F32	 Convert two single-precision floats into a packed FP16 result and round to zero (ignore the current rounding mode). This opcode is intended for use with 16-bit compressed exports. See V_CVT_F16_F32 for a version that respects the current rounding mode. D.f16_lo = f32_to_f16(S0.f32); D.f16_hi = f32_to_f16(S1.f32). // Round-toward-zero regardless of current round mode setting in hardware.
50	V_ADD_F16	Add two FP16 values.  0.5ULP precision.  Supports denormals, round mode, exception flags and saturation. D.f16_lo = S0.f16_lo + S1.f16_lo.
51	V_SUB_F16	Subtract the second FP16 value from the first.  0.5ULP precision, Supports denormals, round mode, exception flags and saturation. D.f16_lo = S0.f16_lo - S1.f16_lo.
52	V_SUBREV_F16	Subtract the first FP16 value from the second.  0.5ULP precision.  Supports denormals, round mode, exception flags and saturation. D.f16_lo = S1.f16_lo - S0.f16_lo.
53	V_MUL_F16	Multiply two FP16 values.  0.5ULP precision.  Supports denormals, round mode, exception flags and saturation. D.f16_lo = S0.f16_lo * S1.f16_lo.
54	V_FMAC_F16	Fused multiply-add of FP16 values, accumulate with destination. 0.5ULP precision.  Supports denormals, round mode, exception flags and saturation. D.f16_lo = S0.f16_lo * S1.f16_lo + D.f16_lo.
55	V_FMAMK_F16	Multiply a FP16 value with a literal constant and add a second FP16 value using fused multiply-add.  This opcode cannot use the VOP3 encoding and cannot use input/output modifiers.  Supports round mode, exception flags, saturation. D.f16_lo = S0.f16_lo * K.f16_lo + S1.f16_lo. // K is a 32-bit literal constant stored in the following literal DWORD.
56	V_FMAAK_F16	Multiply two FP16 values and add a literal constant using fused multiply-add.  This opcode cannot use the VOP3 encoding and cannot use input/output modifiers.  Supports round mode, exception flags, saturation. D.f16_lo = S0.f16_lo * S1.f16_lo + K.f16_lo. // K is a 32-bit literal constant stored in the following literal DWORD.
57	V_MAX_F16	Maximum of two FP16 values.  IEEE compliant.  Supports denormals, round mode, exception flags, saturation. D.f16 = max(S0.f16,S1.f16); if (IEEE_MODE && S0.f16 == sNaN) D.f16 = Quiet(S0.f16); else if (IEEE_MODE && S1.f16 == sNaN) D.f16 = Quiet(S1.f16); else if (S0.f16 == NaN) D.f16 = S1.f16; else if (S1.f16 == NaN) D.f16 = S0.f16; else if (S0.f16 == +0.0 && S1.f16 == -0.0) D.f16 = S0.f16; else if (S0.f16 == -0.0 && S1.f16 == +0.0) D.f16 = S1.f16; else if (IEEE_MODE) D.f16 = (S0.f16 >= S1.f16 ? S0.f16 : S1.f16); else D.f16 = (S0.f16 > S1.f16 ? S0.f16 : S1.f16); endif.
58	V_MIN_F16	Minimum of two FP16 values.  IEEE compliant.  Supports denormals, round mode, exception flags, saturation. D.f16 = min(S0.f16,S1.f16); if (IEEE_MODE && S0.f16 == sNaN) D.f16 = Quiet(S0.f16); else if (IEEE_MODE && S1.f16 == sNaN) D.f16 = Quiet(S1.f16); else if (S0.f16 == NaN) D.f16 = S1.f16; else if (S1.f16 == NaN) D.f16 = S0.f16; else if (S0.f16 == +0.0 && S1.f16 == -0.0) D.f16 = S1.f16; else if (S0.f16 == -0.0 && S1.f16 == +0.0) D.f16 = S0.f16; else // Note: there's no IEEE special case here like there is for V_MAX_F16. D.f16 = (S0.f16 < S1.f16 ? S0.f16 : S1.f16); endif.
59	V_LDEXP_F16	Multiply an FP16 value by an integral power of 2, compare with the ldexp() function in C.  Note that the S1 has a format of f16 since floating point literal constants are interpreted as 16 bit value for this opcode. D.f16 = S0.f16 * (2 ** S1.i16).
60	V_PK_FMAC_F16	Multiply packed FP16 values and accumulate with destination. VOP2 version of V_PK_FMA_F16 with third source VGPR address is the destination. D.f16_lo = S0.f16_lo * S1.f16_lo + D.f16_lo; D.f16_hi = S0.f16_hi * S1.f16_hi + D.f16_hi.

# VOP1 Instructions
0	V_NOP	Do nothing, with style!
1	V_MOV_B32	Move data to a VGPR.  Floating-point modifiers are valid for this instruction if S0.u is a 32-bit floating point value.  This instruction is suitable for negating or taking the absolute value of a floating-point value. D.u = S0.u. Examples: v_mov_b32 v0, v1// Move v1 to v0 v_mov_b32 v0, -v1// Set v1 to the negation of v0 v_mov_b32 v0, abs(v1)// Set v1 to the absolute value of v0
2	V_READFIRSTLANE_B32	Copy one VGPR value to one SGPR.  D = SGPR destination, S0 = source data (VGPR# or M0 for lds direct access), Lane# = FindFirst1fromLSB(exec) (Lane# = 0 if exec is zero).  Ignores exec mask for the access.  Input and output modifiers not supported; this is an untyped operation.
3	V_CVT_I32_F64	Convert from a double-precision float to a signed integer. 0.5ULP accuracy, out-of-range floating point values (including infinity) saturate.  NaN is converted to 0.  Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.i = (int)S0.d.
4	V_CVT_F64_I32	Convert from a signed integer to a double-precision float, 0ULP accuracy. D.d = (double)S0.i.
5	V_CVT_F32_I32	Convert from a signed integer to a single-precision float, 0.5ULP accuracy. D.f = (float)S0.i.
6	V_CVT_F32_U32	Convert from an unsigned integer to a single-precision float, 0.5ULP accuracy. D.f = (float)S0.u.
7	V_CVT_U32_F32	Convert from a single-precision float to an unsigned integer. 1ULP accuracy, out-of-range floating point values (including infinity) saturate.  NaN is converted to 0.  Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.u = (unsigned)S0.f.
8	V_CVT_I32_F32	Convert from a single-precision float to a signed integer.  1ULP accuracy, out-of-range floating point values (including infinity) saturate.  NaN is converted to 0.  Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.i = (int)S0.f.
10	V_CVT_F16_F32	Convert from a single-precision float to an FP16 float.  0.5ULP accuracy, supports input modifiers and creates FP16 denormals when appropriate. D.f16 = flt32_to_flt16(S0.f).
11	V_CVT_F32_F16	Convert from an FP16 float to a single-precision float.  0ULP accuracy, FP16 denormal inputs are accepted. D.f = flt16_to_flt32(S0.f16).
12	V_CVT_RPI_I32_F32	Convert from a single-precision float to a signed integer, round to nearest integer.0.5ULP accuracy, denormals are supported. D.i = (int)floor(S0.f + 0.5).
13	V_CVT_FLR_I32_F32	Convert from a single-precision float to a signed integer, round down.  1ULP accuracy, denormals are supported. D.i = (int)floor(S0.f).
14	V_CVT_OFF_F32_I4	4-bit signed int to 32-bit float.  Used for interpolation in shader. S0___Result__ 1000 -0.5000f 1001 -0.4375f 1010 -0.3750f 1011 -0.3125f 1100 -0.2500f 1101 -0.1875f 1110 -0.1250f 1111 -0.0625f 0000 +0.0000f 0001 +0.0625f 0010 +0.1250f 0011 +0.1875f 0100 +0.2500f 0101 +0.3125f 0110 +0.3750f 0111 +0.4375f
15	V_CVT_F32_F64	Convert from a double-precision float to a single-precision float.  0.5ULP accuracy, denormals are supported. D.f = (float)S0.d.
16	V_CVT_F64_F32	Convert from a single-precision float to a double-precision float. 0ULP accuracy, denormals are supported. D.d = (double)S0.f.
17	V_CVT_F32_UBYTE0	Convert an unsigned byte (byte 0) to a single-precision float. D.f = (float)(S0.u[7:0]).
18	V_CVT_F32_UBYTE1	Convert an unsigned byte (byte 1) to a single-precision float. D.f = (float)(S0.u[15:8]).
19	V_CVT_F32_UBYTE2	Convert an unsigned byte (byte 2) to a single-precision float. D.f = (float)(S0.u[23:16]).
20	V_CVT_F32_UBYTE3	Convert an unsigned byte (byte 3) to a single-precision float. D.f = (float)(S0.u[31:24]).
21	V_CVT_U32_F64	Convert from a double-precision float to an unsigned integer. 0.5ULP accuracy, out-of-range floating point values (including infinity) saturate.  NaN is converted to 0.  Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.u = (unsigned)S0.d.
22	V_CVT_F64_U32	Convert from an unsigned integer to a double-precision float. 0ULP accuracy. D.d = (double)S0.u.
23	V_TRUNC_F64	Return integer part of S0.d, round-to-zero semantics. D.d = trunc(S0.d).
24	V_CEIL_F64	Round up to next whole integer. D.d = trunc(S0.d); if(S0.d > 0.0 && S0.d != D.d) then D.d += 1.0; endif.
25	V_RNDNE_F64	Round-to-nearest-even semantics. D.d = floor(S0.d + 0.5); if(floor(S0.d) is even && fract(S0.d) == 0.5) then D.d -= 1.0; endif.
26	V_FLOOR_F64	Round down to previous whole integer. D.d = trunc(S0.d); if(S0.d < 0.0 && S0.d != D.d) then D.d += -1.0; endif.
27	V_PIPEFLUSH	Flush the VALU destination cache.
32	V_FRACT_F32	Return fractional portion of a number.  0.5ULP accuracy, denormals are accepted. D.f = S0.f + -floor(S0.f). NOTE: This complies with the DX specification of fract where the function behaves like an extension of integer modulus; be aware this may differ from how fract() is defined in other domains. For example: fract(-1.2) = 0.8 in DX.
33	V_TRUNC_F32	Return integer part of S0.f, round-to-zero semantics. D.f = trunc(S0.f).
34	V_CEIL_F32	Round up to next whole integer. D.f = trunc(S0.f); if(S0.f > 0.0 && S0.f != D.f) then D.f += 1.0; endif.
35	V_RNDNE_F32	Round-to-nearest-even semantics. D.f = floor(S0.f + 0.5); if(floor(S0.f) is even && fract(S0.f) == 0.5) then D.f -= 1.0; endif.
36	V_FLOOR_F32	Round down to previous whole integer. D.f = trunc(S0.f); if(S0.f < 0.0 && S0.f != D.f) then D.f += -1.0; endif.
37	V_EXP_F32	Base 2 exponentiation.  1ULP accuracy, denormals are flushed. D.f = pow(2.0, S0.f). Functional examples: V_EXP_F32(0xff800000) => 0x00000000    // exp(-INF) = 0 V_EXP_F32(0x80000000) => 0x3f800000    // exp(-0.0) = 1 V_EXP_F32(0x7f800000) => 0x7f800000    // exp(+INF) = +INF
39	V_LOG_F32	Base 2 logarithm.  1ULP accuracy, denormals are flushed. D.f = log2(S0.f). Functional examples: V_LOG_F32(0xff800000) => 0xffc00000    // log(-INF) = NAN V_LOG_F32(0xbf800000) => 0xffc00000    // log(-1.0) = NAN V_LOG_F32(0x80000000) => 0xff800000    // log(-0.0) = -INF V_LOG_F32(0x00000000) => 0xff800000    // log(+0.0) = -INF V_LOG_F32(0x3f800000) => 0x00000000    // log(+1.0) = 0 V_LOG_F32(0x7f800000) => 0x7f800000    // log(+INF) = +INF
42	V_RCP_F32	Compute reciprocal with IEEE rules.  1ULP accuracy.  Accuracy converges to < 0.5ULP when using the Newton-Raphson method and 2 FMA operations.  Denormals are flushed. D.f = 1.0 / S0.f. Functional examples: V_RCP_F32(0xff800000) => 0x80000000    // rcp(-INF) = -0 V_RCP_F32(0xc0000000) => 0xbf000000    // rcp(-2.0) = -0.5 V_RCP_F32(0x80000000) => 0xff800000    // rcp(-0.0) = -INF V_RCP_F32(0x00000000) => 0x7f800000    // rcp(+0.0) = +INF V_RCP_F32(0x7f800000) => 0x00000000    // rcp(+INF) = +0
43	V_RCP_IFLAG_F32	Compute reciprocal as part of integer divide.  Can raise integer DIV_BY_ZERO exception but cannot raise floating-point exceptions.  To be used in an integer reciprocal macro by the compiler with one of the following sequences: D.f = 1.0 / S0.f. Unsigned usage: CVT_F32_U32 RCP_IFLAG_F32 MUL_F32 (2**32 - 1) CVT_U32_F32 Signed usage: CVT_F32_I32 RCP_IFLAG_F32 MUL_F32 (2**31 - 1) CVT_I32_F32
46	V_RSQ_F32	Reciprocal square root with IEEE rules.  1ULP accuracy, denormals are flushed. D.f = 1.0 / sqrt(S0.f). Functional examples: V_RSQ_F32(0xff800000) => 0xffc00000    // rsq(-INF) = NAN V_RSQ_F32(0x80000000) => 0xff800000    // rsq(-0.0) = -INF V_RSQ_F32(0x00000000) => 0x7f800000    // rsq(+0.0) = +INF V_RSQ_F32(0x40800000) => 0x3f000000    // rsq(+4.0) = +0.5 V_RSQ_F32(0x7f800000) => 0x00000000    // rsq(+INF) = +0
47	V_RCP_F64	Reciprocal with IEEE rules.  Precision is (2**29) ULP, and supports denormals. D.d = 1.0 / S0.d.
49	V_RSQ_F64	Reciprocal square root with IEEE rules.  Precision is (2**29) ULP, and supports denormals. D.f16 = 1.0 / sqrt(S0.f16).
51	V_SQRT_F32	Square root.  1ULP accuracy, denormals are flushed. D.f = sqrt(S0.f). Functional examples: V_SQRT_F32(0xff800000) => 0xffc00000    // sqrt(-INF) = NAN V_SQRT_F32(0x80000000) => 0x80000000    // sqrt(-0.0) = -0 V_SQRT_F32(0x00000000) => 0x00000000    // sqrt(+0.0) = +0 V_SQRT_F32(0x40800000) => 0x40000000    // sqrt(+4.0) = +2.0 V_SQRT_F32(0x7f800000) => 0x7f800000    // sqrt(+INF) = +INF
52	V_SQRT_F64	Square root.  Precision is (2**29) ULP, and supports denormals. D.d = sqrt(S0.d).
53	V_SIN_F32	Trigonometric sine.  Denormals are supported. D.f = sin(S0.f * 2 * PI). Functional examples: V_SIN_F32(0xff800000) => 0xffc00000    // sin(-INF) = NAN V_SIN_F32(0xff7fffff) => 0x00000000    // -MaxFloat, finite V_SIN_F32(0x80000000) => 0x80000000    // sin(-0.0) = -0 V_SIN_F32(0x3e800000) => 0x3f800000    // sin(0.25) = 1 V_SIN_F32(0x7f800000) => 0xffc00000    // sin(+INF) = NAN
54	V_COS_F32	Trigonometric cosine.  Denormals are supported. D.f = cos(S0.f * 2 * PI). Functional examples: V_COS_F32(0xff800000) => 0xffc00000    // cos(-INF) = NAN V_COS_F32(0xff7fffff) => 0x3f800000    // -MaxFloat, finite V_COS_F32(0x80000000) => 0x3f800000    // cos(-0.0) = 1 V_COS_F32(0x3e800000) => 0x00000000    // cos(0.25) = 0 V_COS_F32(0x7f800000) => 0xffc00000    // cos(+INF) = NAN
55	V_NOT_B32	Bitwise negation.  Input and output modifiers not supported. D.u = ~S0.u.
56	V_BFREV_B32	Bitfield reverse.  Input and output modifiers not supported. D.u[31:0] = S0.u[0:31].
57	V_FFBH_U32	Counts how many zeros before the first one starting from the MSB. Returns -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... 31 do // Note: search is from the MSB if S0.u[31 - i] == 1 then D.i = i; break for; endif; endfor. Functional examples: V_FFBH_U32(0x00000000) => 0xffffffff V_FFBH_U32(0x800000ff) => 0 V_FFBH_U32(0x100000ff) => 3 V_FFBH_U32(0x0000ffff) => 16 V_FFBH_U32(0x00000001) => 31
58	V_FFBL_B32	Returns the bit position of the first one from the LSB, or -1 if there are no ones. D.i = -1; // Set if no ones are found for i in 0 ... 31 do // Search from LSB if S0.u[i] == 1 then D.i = i; break for; endif; endfor. Functional examples: V_FFBL_B32(0x00000000) => 0xffffffff V_FFBL_B32(0xff000001) => 0 V_FFBL_B32(0xff000008) => 3 V_FFBL_B32(0xffff0000) => 16 V_FFBL_B32(0x80000000) => 31
59	V_FFBH_I32	Counts how many bits in a row (from MSB to LSB) are the same as the sign bit.  Returns -1 if all bits are the same. D.i = -1; // Set if all bits are the same for i in 1 ... 31 do // Note: search is from the MSB if S0.i[31 - i] != S0.i[31] then D.i = i; break for; endif; endfor. Functional examples: V_FFBH_I32(0x00000000) => 0xffffffff V_FFBH_I32(0x40000000) => 1 V_FFBH_I32(0x80000000) => 1 V_FFBH_I32(0x0fffffff) => 4 V_FFBH_I32(0xffff0000) => 16 V_FFBH_I32(0xfffffffe) => 31 V_FFBH_I32(0xffffffff) => 0xffffffff
60	V_FREXP_EXP_I32_F6	Returns exponent of single precision float input, such that S0.d 4 = significand * (2 ** exponent).  See also V_FREXP_MANT_F64, which returns the significand.  See the C library function frexp() for more information. if(S0.f64 == +-INF || S0.f64 == NAN) D.i32 = 0; else D.i32 = S0.f64.exp - 1023 + 1; endif.
61	V_FREXP_MANT_F64	Returns binary significand of double precision float input, such that S0.d = significand * (2 ** exponent).  Result range is in (-1.0,-0.5][0.5,1.0) in normal cases.  See also V_FREXP_EXP_I32_F64, which returns integer exponent.  See the C library function frexp() for more information. if(S0.d == +-INF || S0.d == NAN) then D.d = S0.d; else D.d = Mantissa(S0.d); endif.
62	V_FRACT_F64	Return fractional portion of a number.  0.5ULP accuracy, denormals are accepted. D.d = S0.d + -floor(S0.d). NOTE: This complies with the DX specification of fract where the function behaves like an extension of integer modulus; be aware this may differ from how fract() is defined in other domains. For example: fract(-1.2) = 0.8 in DX.
63	V_FREXP_EXP_I32_F3	Returns exponent of single precision float input, such that S0.f 2 = significand * (2 ** exponent). See also V_FREXP_MANT_F32, which returns the significand.  See the C library function frexp() for more information. if(S0.f == +-INF || S0.f == NAN) then D.i = 0; else D.i = TwosComplement(Exponent(S0.f) - 127 + 1); endif.
64	V_FREXP_MANT_F32	Returns binary significand of single precision float input, such that S0.f = significand * (2 ** exponent).  Result range is in (-1.0,-0.5][0.5,1.0) in normal cases.  See also V_FREXP_EXP_I32_F32, which returns integer exponent.  See the C library function frexp() for more information. if(S0.f == +-INF || S0.f == NAN) then D.f = S0.f; else D.f = Mantissa(S0.f); endif.
65	V_CLREXCP	Clear this wave's exception state in the SIMD (SP).
66	V_MOVRELD_B32	Move to a relative destination address. addr = VGPR address appearing in instruction DST field; addr += M0.u[31:0]; VGPR[addr].u = S0.u. Example: The following instruction sequence will perform a move v15 <== v7: s_mov_b32 m0, 10 v_movreld_b32 v5, v7
67	V_MOVRELS_B32	Move from a relative source address. addr = VGPR address appearing in instruction SRC0 field; addr += M0.u[31:0]; D.u = VGPR[addr].u. Example: The following instruction sequence will perform a move v5 <== v17: s_mov_b32 m0, 10 v_movrels_b32 v5, v7
68	V_MOVRELSD_B32	Move from a relative source address to a relative destination address. addr_src = VGPR address appearing in instruction SRC0 field; addr_src += M0.u[31:0]; addr_dst = VGPR address appearing in instruction DST field; addr_dst += M0.u[31:0]; VGPR[addr_dst].u = VGPR[addr_src].u. Example: The following instruction sequence will perform a move v15 <== v17: s_mov_b32 m0, 10 v_movrelsd_b32 v5, v7
72	V_MOVRELSD_2_B32	Move from a relative source address to a relative destination address, with different relative offsets. addr_src = VGPR address appearing in instruction SRC0 field; addr_src += M0.u[9:0]; addr_dst = VGPR address appearing in instruction DST field; addr_dst += M0.u[25:16]; VGPR[addr_dst].u = VGPR[addr_src].u. Example: The following instruction sequence will perform a move v25 <== v17: s_mov_b32 m0, ((20 << 16) | 10) v_movrelsd_2_b32 v5, v7
80	V_CVT_F16_U16	Convert from an unsigned short to an FP16 float.  0.5ULP accuracy, supports denormals, rounding, exception flags and saturation. D.f16 = uint16_to_flt16(S.u16).
81	V_CVT_F16_I16	Convert from a signed short to an FP16 float.  0.5ULP accuracy, supports denormals, rounding, exception flags and saturation. D.f16 = int16_to_flt16(S.i16).
82	V_CVT_U16_F16	Convert from an FP16 float to an unsigned short.  1ULP accuracy, supports rounding, exception flags and saturation.  FP16 denormals are accepted.  Conversion is done with truncation. Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.u16 = flt16_to_uint16(S.f16).
83	V_CVT_I16_F16	Convert from an FP16 float to a signed short.  1ULP accuracy, supports rounding, exception flags and saturation.  FP16 denormals are accepted.  Conversion is done with truncation. Generation of the INEXACT exception is controlled by the CLAMP bit.  INEXACT exceptions are enabled for this conversion iff CLAMP == 1. D.i16 = flt16_to_int16(S.f16).
84	V_RCP_F16	Reciprocal with IEEE rules. 0.51ULP accuracy. D.f16 = 1.0 / S0.f16. Functional examples: V_RCP_F16(0xfc00) => 0x8000    // rcp(-INF) = -0 V_RCP_F16(0xc000) => 0xb800    // rcp(-2.0) = -0.5 V_RCP_F16(0x8000) => 0xfc00    // rcp(-0.0) = -INF V_RCP_F16(0x0000) => 0x7c00    // rcp(+0.0) = +INF V_RCP_F16(0x7c00) => 0x0000    // rcp(+INF) = +0
85	V_SQRT_F16	Square root. 0.51ULP accuracy, denormals are supported. D.f16 = sqrt(S0.f16). Functional examples: V_SQRT_F16(0xfc00) => 0xfe00    // sqrt(-INF) = NAN V_SQRT_F16(0x8000) => 0x8000    // sqrt(-0.0) = -0 V_SQRT_F16(0x0000) => 0x0000    // sqrt(+0.0) = +0 V_SQRT_F16(0x4400) => 0x4000    // sqrt(+4.0) = +2.0 V_SQRT_F16(0x7c00) => 0x7c00    // sqrt(+INF) = +INF
86	V_RSQ_F16	Reciprocal square root with IEEE rules.  0.51ULP accuracy, denormals are supported. D.f16 = 1.0 / sqrt(S0.f16). Functional examples: V_RSQ_F16(0xfc00) => 0xfe00    // rsq(-INF) = NAN V_RSQ_F16(0x8000) => 0xfc00    // rsq(-0.0) = -INF V_RSQ_F16(0x0000) => 0x7c00    // rsq(+0.0) = +INF V_RSQ_F16(0x4400) => 0x3800    // rsq(+4.0) = +0.5 V_RSQ_F16(0x7c00) => 0x0000    // rsq(+INF) = +0
87	V_LOG_F16	Base 2 logarithm.  0.51ULP accuracy, denormals are supported. D.f16 = log2(S0.f). Functional examples: V_LOG_F16(0xfc00) => 0xfe00    // log(-INF) = NAN V_LOG_F16(0xbc00) => 0xfe00    // log(-1.0) = NAN V_LOG_F16(0x8000) => 0xfc00    // log(-0.0) = -INF V_LOG_F16(0x0000) => 0xfc00    // log(+0.0) = -INF V_LOG_F16(0x3c00) => 0x0000    // log(+1.0) = 0 V_LOG_F16(0x7c00) => 0x7c00    // log(+INF) = +INF
88	V_EXP_F16	Base 2 exponentiation.  0.51ULP accuracy, denormals are supported. D.f16 = pow(2.0, S0.f16). Functional examples: V_EXP_F16(0xfc00) => 0x0000// exp(-INF) = 0 V_EXP_F16(0x8000) => 0x3c00// exp(-0.0) = 1 V_EXP_F16(0x7c00) => 0x7c00// exp(+INF) = +INF
89	V_FREXP_MANT_F16	Returns binary significand of half precision float input, such that S0.f16 = significand * (2 ** exponent).  Result range is in (-1.0,-0.5][0.5,1.0) in normal cases.  See also V_FREXP_EXP_I16_F16, which returns integer exponent.  See the C library function frexp() for more information. if(S0.f16 == +-INF || S0.f16 == NAN) then D.f16 = S0.f16; else D.f16 = Mantissa(S0.f16); endif.
90	V_FREXP_EXP_I16_F1	Returns exponent of half precision float input, such that S0.f16 6 = significand * (2 ** exponent). See also V_FREXP_MANT_F16, which returns the significand.  See the C library function frexp() for more information. if(S0.f16 == +-INF || S0.f16 == NAN) then D.i = 0; else D.i = TwosComplement(Exponent(S0.f16) - 15 + 1); endif.
91	V_FLOOR_F16	Round down to previous whole integer. D.f16 = trunc(S0.f16); if(S0.f16 < 0.0f && S0.f16 != D.f16) then D.f16 -= 1.0; endif.
92	V_CEIL_F16	Round up to next whole integer. D.f16 = trunc(S0.f16); if(S0.f16 > 0.0f && S0.f16 != D.f16) then D.f16 += 1.0; endif.
93	V_TRUNC_F16	Return integer part of S0.f16, round-to-zero semantics. D.f16 = trunc(S0.f16).
94	V_RNDNE_F16	Round-to-nearest-even semantics. D.f16 = floor(S0.f16 + 0.5); if(floor(S0.f16) is even && fract(S0.f16) == 0.5) then D.f16 -= 1.0; endif.
95	V_FRACT_F16	Return fractional portion of a number.  0.5ULP accuracy, denormals are accepted. D.f16 = S0.f16 + -floor(S0.f16). NOTE: This complies with the DX specification of fract where the function behaves like an extension of integer modulus; be aware this may differ from how fract() is defined in other domains. For example: fract(-1.2) = 0.8 in DX.
96	V_SIN_F16	Trigonometric sine.  Denormals are supported. D.f16 = sin(S0.f16 * 2 * PI). Functional examples: V_SIN_F16(0xfc00) => 0xfe00    // sin(-INF) = NAN V_SIN_F16(0xfbff) => 0x0000    // Most negative finite FP16 V_SIN_F16(0x8000) => 0x8000    // sin(-0.0) = -0 V_SIN_F16(0x3400) => 0x3c00    // sin(0.25) = 1 V_SIN_F16(0x7bff) => 0x0000    // Most positive finite FP16 V_SIN_F16(0x7c00) => 0xfe00    // sin(+INF) = NAN
97	V_COS_F16	Trigonometric cosine.  Denormals are supported. D.f16 = cos(S0.f16 * 2 * PI). Functional examples: V_COS_F16(0xfc00) => 0xfe00    // cos(-INF) = NAN V_COS_F16(0xfbff) => 0x3c00    // Most negative finite FP16 V_COS_F16(0x8000) => 0x3c00    // cos(-0.0) = 1 V_COS_F16(0x3400) => 0x0000    // cos(0.25) = 0 V_COS_F16(0x7bff) => 0x3c00    // Most positive finite FP16 V_COS_F16(0x7c00) => 0xfe00    // cos(+INF) = NAN
98	V_SAT_PK_U8_I16	Packed 8-bit saturating add. D.u32 = {16'b0, sat8(S.u[31:16]), sat8(S.u[15:0])}.
99	V_CVT_NORM_I16_F16	 Convert from an FP16 float to a signed normalized short.  0.5ULP accuracy, supports rounding, exception flags and saturation, denormals are supported. D.i16 = flt16_to_snorm16(S.f16).
100	V_CVT_NORM_U16_F16	 Convert from an FP16 float to an unsigned normalized short. 0.5ULP accuracy, supports rounding, exception flags and saturation, denormals are supported. D.u16 = flt16_to_unorm16(S.f16).
101	V_SWAP_B32	Swap operands.  Input and output modifiers not supported; this is an untyped operation. tmp = D.u; D.u = S0.u; S0.u = tmp.

# VOPC Instructions
0	V_CMP_F_F32	D[threadId] = 0. // D = VCC in VOPC encoding.
1	V_CMP_LT_F32	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
2	V_CMP_EQ_F32	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
3	V_CMP_LE_F32	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
4	V_CMP_GT_F32	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
5	V_CMP_LG_F32	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
6	V_CMP_GE_F32	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
7	V_CMP_O_F32	D[threadId] = (!isNan(S0) && !isNan(S1)). // D = VCC in VOPC encoding.
8	V_CMP_U_F32	D[threadId] = (isNan(S0)  ||  isNan(S1)). // D = VCC in VOPC encoding.
9	V_CMP_NGE_F32	D[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <. // D = VCC in VOPC encoding.
10	V_CMP_NLG_F32	D[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==. // D = VCC in VOPC encoding.
11	V_CMP_NGT_F32	D[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=. // D = VCC in VOPC encoding.
12	V_CMP_NLE_F32	D[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >. // D = VCC in VOPC encoding.
13	V_CMP_NEQ_F32	D[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=. // D = VCC in VOPC encoding.
14	V_CMP_NLT_F32	D[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=. // D = VCC in VOPC encoding.
15	V_CMP_TRU_F32	D[threadId] = 1. // D = VCC in VOPC encoding.
16	V_CMPX_F_F32	EXEC[threadId] = 0.
17	V_CMPX_LT_F32	EXEC[threadId] = (S0 < S1).
18	V_CMPX_EQ_F32	EXEC[threadId] = (S0 == S1).
19	V_CMPX_LE_F32	EXEC[threadId] = (S0 <= S1).
20	V_CMPX_GT_F32	EXEC[threadId] = (S0 > S1).
21	V_CMPX_LG_F32	EXEC[threadId] = (S0 <> S1).
22	V_CMPX_GE_F32	EXEC[threadId] = (S0 >= S1).
23	V_CMPX_O_F32	EXEC[threadId] = (!isNan(S0) && !isNan(S1)).
24	V_CMPX_U_F32	EXEC[threadId] = (isNan(S0)  ||  isNan(S1)).
25	V_CMPX_NGE_F32	EXEC[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <.
26	V_CMPX_NLG_F32	EXEC[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==.
27	V_CMPX_NGT_F32	EXEC[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=.
28	V_CMPX_NLE_F32	EXEC[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >.
29	V_CMPX_NEQ_F32	EXEC[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=.
30	V_CMPX_NLT_F32	EXEC[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=.
31	V_CMPX_TRU_F32	EXEC[threadId] = 1.
32	V_CMP_F_F64	D[threadId] = 0. // D = VCC in VOPC encoding.
33	V_CMP_LT_F64	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
34	V_CMP_EQ_F64	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
35	V_CMP_LE_F64	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
36	V_CMP_GT_F64	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
37	V_CMP_LG_F64	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
38	V_CMP_GE_F64	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
39	V_CMP_O_F64	D[threadId] = (!isNan(S0) && !isNan(S1)). // D = VCC in VOPC encoding.
40	V_CMP_U_F64	D[threadId] = (isNan(S0)  ||  isNan(S1)). // D = VCC in VOPC encoding.
41	V_CMP_NGE_F64	D[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <. // D = VCC in VOPC encoding.
42	V_CMP_NLG_F64	D[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==. // D = VCC in VOPC encoding.
43	V_CMP_NGT_F64	D[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=. // D = VCC in VOPC encoding.
44	V_CMP_NLE_F64	D[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >. // D = VCC in VOPC encoding.
45	V_CMP_NEQ_F64	D[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=. // D = VCC in VOPC encoding.
46	V_CMP_NLT_F64	D[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=. // D = VCC in VOPC encoding.
47	V_CMP_TRU_F64	D[threadId] = 1. // D = VCC in VOPC encoding.
48	V_CMPX_F_F64	EXEC[threadId] = 0.
49	V_CMPX_LT_F64	EXEC[threadId] = (S0 < S1).
50	V_CMPX_EQ_F64	EXEC[threadId] = (S0 == S1).
51	V_CMPX_LE_F64	EXEC[threadId] = (S0 <= S1).
52	V_CMPX_GT_F64	EXEC[threadId] = (S0 > S1).
53	V_CMPX_LG_F64	EXEC[threadId] = (S0 <> S1).
54	V_CMPX_GE_F64	EXEC[threadId] = (S0 >= S1).
55	V_CMPX_O_F64	EXEC[threadId] = (!isNan(S0) && !isNan(S1)).
56	V_CMPX_U_F64	EXEC[threadId] = (isNan(S0)  ||  isNan(S1)).
57	V_CMPX_NGE_F64	EXEC[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <.
58	V_CMPX_NLG_F64	EXEC[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==.
59	V_CMPX_NGT_F64	EXEC[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=.
60	V_CMPX_NLE_F64	EXEC[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >.
61	V_CMPX_NEQ_F64	EXEC[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=.
62	V_CMPX_NLT_F64	EXEC[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=.
63	V_CMPX_TRU_F64	EXEC[threadId] = 1.
128	V_CMP_F_I32	D[threadId] = 0. // D = VCC in VOPC encoding.
129	V_CMP_LT_I32	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
130	V_CMP_EQ_I32	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
131	V_CMP_LE_I32	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
132	V_CMP_GT_I32	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
133	V_CMP_NE_I32	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
134	V_CMP_GE_I32	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
135	V_CMP_T_I32	D[threadId] = 1. // D = VCC in VOPC encoding.
136	V_CMP_CLASS_F32	VCC = IEEE numeric class function specified in S1.u, performed on S0.f. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
137	V_CMP_LT_I16	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
138	V_CMP_EQ_I16	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
139	V_CMP_LE_I16	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
140	V_CMP_GT_I16	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
141	V_CMP_NE_I16	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
142	V_CMP_GE_I16	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
143	V_CMP_CLASS_F16	VCC = IEEE numeric class function specified in S1.u, performed on S0.f16. Note that the S1 has a format of f16 since floating point literal constants are interpreted as 16 bit value for this opcode. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
144	V_CMPX_F_I32	EXEC[threadId] = 0.
145	V_CMPX_LT_I32	EXEC[threadId] = (S0 < S1).
146	V_CMPX_EQ_I32	EXEC[threadId] = (S0 == S1).
147	V_CMPX_LE_I32	EXEC[threadId] = (S0 <= S1).
148	V_CMPX_GT_I32	EXEC[threadId] = (S0 > S1).
149	V_CMPX_NE_I32	EXEC[threadId] = (S0 <> S1).
150	V_CMPX_GE_I32	EXEC[threadId] = (S0 >= S1).
151	V_CMPX_T_I32	EXEC[threadId] = 1.
152	V_CMPX_CLASS_F32	EXEC = IEEE numeric class function specified in S1.u, performed on S0.f. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
153	V_CMPX_LT_I16	EXEC[threadId] = (S0 < S1).
154	V_CMPX_EQ_I16	EXEC[threadId] = (S0 == S1).
155	V_CMPX_LE_I16	EXEC[threadId] = (S0 <= S1).
156	V_CMPX_GT_I16	EXEC[threadId] = (S0 > S1).
157	V_CMPX_NE_I16	EXEC[threadId] = (S0 <> S1).
158	V_CMPX_GE_I16	EXEC[threadId] = (S0 >= S1).
159	V_CMPX_CLASS_F16	EXEC = IEEE numeric class function specified in S1.u, performed on S0.f16. Note that the S1 has a format of f16 since floating point literal constants are interpreted as 16 bit value for this opcode. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
160	V_CMP_F_I64	D[threadId] = 0. // D = VCC in VOPC encoding.
161	V_CMP_LT_I64	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
162	V_CMP_EQ_I64	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
163	V_CMP_LE_I64	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
164	V_CMP_GT_I64	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
165	V_CMP_NE_I64	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
166	V_CMP_GE_I64	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
167	V_CMP_T_I64	D[threadId] = 1. // D = VCC in VOPC encoding.
168	V_CMP_CLASS_F64	VCC = IEEE numeric class function specified in S1.u, performed on S0.d. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
169	V_CMP_LT_U16	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
170	V_CMP_EQ_U16	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
171	V_CMP_LE_U16	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
172	V_CMP_GT_U16	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
173	V_CMP_NE_U16	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
174	V_CMP_GE_U16	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
176	V_CMPX_F_I64	EXEC[threadId] = 0.
177	V_CMPX_LT_I64	EXEC[threadId] = (S0 < S1).
178	V_CMPX_EQ_I64	EXEC[threadId] = (S0 == S1).
179	V_CMPX_LE_I64	EXEC[threadId] = (S0 <= S1).
180	V_CMPX_GT_I64	EXEC[threadId] = (S0 > S1).
181	V_CMPX_NE_I64	EXEC[threadId] = (S0 <> S1).
182	V_CMPX_GE_I64	EXEC[threadId] = (S0 >= S1).
183	V_CMPX_T_I64	EXEC[threadId] = 1.
184	V_CMPX_CLASS_F64	EXEC = IEEE numeric class function specified in S1.u, performed on S0.d. The function reports true if the floating point value is *any* of the numeric types selected in S1.u according to the following list: S1.u[0] -- value is a signaling NaN. S1.u[1] -- value is a quiet NaN. S1.u[2] -- value is negative infinity. S1.u[3] -- value is a negative normal value. S1.u[4] -- value is a negative denormal value. S1.u[5] -- value is negative zero. S1.u[6] -- value is positive zero. S1.u[7] -- value is a positive denormal value. S1.u[8] -- value is a positive normal value. S1.u[9] -- value is positive infinity.
185	V_CMPX_LT_U16	EXEC[threadId] = (S0 < S1).
186	V_CMPX_EQ_U16	EXEC[threadId] = (S0 == S1).
187	V_CMPX_LE_U16	EXEC[threadId] = (S0 <= S1).
188	V_CMPX_GT_U16	EXEC[threadId] = (S0 > S1).
189	V_CMPX_NE_U16	EXEC[threadId] = (S0 <> S1).
190	V_CMPX_GE_U16	EXEC[threadId] = (S0 >= S1).
192	V_CMP_F_U32	D[threadId] = 0. // D = VCC in VOPC encoding.
193	V_CMP_LT_U32	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
194	V_CMP_EQ_U32	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
195	V_CMP_LE_U32	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
196	V_CMP_GT_U32	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
197	V_CMP_NE_U32	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
198	V_CMP_GE_U32	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
199	V_CMP_T_U32	D[threadId] = 1. // D = VCC in VOPC encoding.
200	V_CMP_F_F16	D[threadId] = 0. // D = VCC in VOPC encoding.
201	V_CMP_LT_F16	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
202	V_CMP_EQ_F16	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
203	V_CMP_LE_F16	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
204	V_CMP_GT_F16	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
205	V_CMP_LG_F16	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
206	V_CMP_GE_F16	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
207	V_CMP_O_F16	D[threadId] = (!isNan(S0) && !isNan(S1)). // D = VCC in VOPC encoding.
208	V_CMPX_F_U32	EXEC[threadId] = 0.
209	V_CMPX_LT_U32	EXEC[threadId] = (S0 < S1).
210	V_CMPX_EQ_U32	EXEC[threadId] = (S0 == S1).
211	V_CMPX_LE_U32	EXEC[threadId] = (S0 <= S1).
212	V_CMPX_GT_U32	EXEC[threadId] = (S0 > S1).
213	V_CMPX_NE_U32	EXEC[threadId] = (S0 <> S1).
214	V_CMPX_GE_U32	EXEC[threadId] = (S0 >= S1).
215	V_CMPX_T_U32	EXEC[threadId] = 1.
216	V_CMPX_F_F16	EXEC[threadId] = 0.
217	V_CMPX_LT_F16	EXEC[threadId] = (S0 < S1).
218	V_CMPX_EQ_F16	EXEC[threadId] = (S0 == S1).
219	V_CMPX_LE_F16	EXEC[threadId] = (S0 <= S1).
220	V_CMPX_GT_F16	EXEC[threadId] = (S0 > S1).
221	V_CMPX_LG_F16	EXEC[threadId] = (S0 <> S1).
222	V_CMPX_GE_F16	EXEC[threadId] = (S0 >= S1).
223	V_CMPX_O_F16	EXEC[threadId] = (!isNan(S0) && !isNan(S1)).
224	V_CMP_F_U64	D[threadId] = 0. // D = VCC in VOPC encoding.
225	V_CMP_LT_U64	D[threadId] = (S0 < S1). // D = VCC in VOPC encoding.
226	V_CMP_EQ_U64	D[threadId] = (S0 == S1). // D = VCC in VOPC encoding.
227	V_CMP_LE_U64	D[threadId] = (S0 <= S1). // D = VCC in VOPC encoding.
228	V_CMP_GT_U64	D[threadId] = (S0 > S1). // D = VCC in VOPC encoding.
229	V_CMP_NE_U64	D[threadId] = (S0 <> S1). // D = VCC in VOPC encoding.
230	V_CMP_GE_U64	D[threadId] = (S0 >= S1). // D = VCC in VOPC encoding.
231	V_CMP_T_U64	D[threadId] = 1. // D = VCC in VOPC encoding.
232	V_CMP_U_F16	D[threadId] = (isNan(S0)  ||  isNan(S1)). // D = VCC in VOPC encoding.
233	V_CMP_NGE_F16	D[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <. // D = VCC in VOPC encoding.
234	V_CMP_NLG_F16	D[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==. // D = VCC in VOPC encoding.
235	V_CMP_NGT_F16	D[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=. // D = VCC in VOPC encoding.
236	V_CMP_NLE_F16	D[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >. // D = VCC in VOPC encoding.
237	V_CMP_NEQ_F16	D[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=. // D = VCC in VOPC encoding.
238	V_CMP_NLT_F16	D[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=. // D = VCC in VOPC encoding.
239	V_CMP_TRU_F16	D[threadId] = 1. // D = VCC in VOPC encoding.
240	V_CMPX_F_U64	EXEC[threadId] = 0.
241	V_CMPX_LT_U64	EXEC[threadId] = (S0 < S1).
242	V_CMPX_EQ_U64	EXEC[threadId] = (S0 == S1).
243	V_CMPX_LE_U64	EXEC[threadId] = (S0 <= S1).
244	V_CMPX_GT_U64	EXEC[threadId] = (S0 > S1).
245	V_CMPX_NE_U64	EXEC[threadId] = (S0 <> S1).
246	V_CMPX_GE_U64	EXEC[threadId] = (S0 >= S1).
247	V_CMPX_T_U64	EXEC[threadId] = 1.
248	V_CMPX_U_F16	EXEC[threadId] = (isNan(S0)  ||  isNan(S1)).
249	V_CMPX_NGE_F16	EXEC[threadId] = !(S0 >= S1) // With NAN inputs this is not the same operation as <.
250	V_CMPX_NLG_F16	EXEC[threadId] = !(S0 <> S1) // With NAN inputs this is not the same operation as ==.
251	V_CMPX_NGT_F16	EXEC[threadId] = !(S0 > S1) // With NAN inputs this is not the same operation as <=.
252	V_CMPX_NLE_F16	EXEC[threadId] = !(S0 <= S1) // With NAN inputs this is not the same operation as >.
253	V_CMPX_NEQ_F16	EXEC[threadId] = !(S0 == S1) // With NAN inputs this is not the same operation as !=.
254	V_CMPX_NLT_F16	EXEC[threadId] = !(S0 < S1) // With NAN inputs this is not the same operation as >=.
255	V_CMPX_TRU_F16	EXEC[threadId] = 1.

# VOP3P Instructions
0	V_PK_MAD_I16	Packed multiply-add on signed shorts. D.i[31:16] = S0.i[31:16] * S1.i[31:16] + S2.i[31:16]; D.i[15:0]  = S0.i[15:0]  * S1.i[15:0]  + S2.i[15:0].
1	V_PK_MUL_LO_U16	Packed multiply on unsigned shorts. D.u[31:16] = S0.u[31:16] * S1.u[31:16]; D.u[15:0]  = S0.u[15:0]  * S1.u[15:0].
2	V_PK_ADD_I16	Packed addition on signed shorts. D.i[31:16] = S0.i[31:16] + S1.i[31:16]; D.i[15:0]  = S0.i[15:0]  + S1.i[15:0].
3	V_PK_SUB_I16	Packed subtraction on signed shorts.  The second operand is subtracted from the first. D.i[31:16] = S0.i[31:16] - S1.i[31:16]; D.i[15:0]  = S0.i[15:0]  - S1.i[15:0].
4	V_PK_LSHLREV_B16	Packed logical shift left.  The shift count is in the first operand. D.u[31:16] = S1.u[31:16] << S0.u[19:16]; D.u[15:0]  = S1.u[15:0]  << S0.u[3:0].
5	V_PK_LSHRREV_B16	Packed logical shift right.  The shift count is in the first operand. D.u[31:16] = S1.u[31:16] >> S0.u[19:16]; D.u[15:0]  = S1.u[15:0]  >> S0.u[3:0].
6	V_PK_ASHRREV_I16	Packed arithmetic shift right (preserve sign bit).  The shift count is in the first operand. D.i[31:16] = S1.i[31:16] >> S0.i[19:16]; D.i[15:0]  = S1.i[15:0]  >> S0.i[3:0].
7	V_PK_MAX_I16	Packed maximum of signed shorts. D.i[31:16] = (S0.i[31:16] >= S1.i[31:16]) ? S0.i[31:16] : S1.i[31:16]; D.i[15:0]  = (S0.i[15:0]  >= S1.i[15:0])  ? S0.i[15:0]  : S1.i[15:0].
8	V_PK_MIN_I16	Packed minimum of signed shorts. D.i[31:16] = (S0.i[31:16] < S1.i[31:16]) ? S0.i[31:16] : S1.i[31:16]; D.i[15:0]  = (S0.i[15:0]  < S1.i[15:0])  ? S0.i[15:0]  : S1.i[15:0].
9	V_PK_MAD_U16	Packed multiply-add on unsigned shorts. D.u[31:16] = S0.u[31:16] * S1.u[31:16] + S2.u[31:16]; D.u[15:0]  = S0.u[15:0]  * S1.u[15:0]  + S2.u[15:0].
10	V_PK_ADD_U16	Packed addition on unsigned shorts. D.u[31:16] = S0.u[31:16] + S1.u[31:16]; D.u[15:0]  = S0.u[15:0]  + S1.u[15:0].
11	V_PK_SUB_U16	Packed subtraction on unsigned shorts.  The second operand is subtracted from the first. D.u[31:16] = S0.u[31:16] - S1.u[31:16]; D.u[15:0]  = S0.u[15:0]  - S1.u[15:0].
12	V_PK_MAX_U16	Packed maximum of unsigned shorts. D.u[31:16] = (S0.u[31:16] >= S1.u[31:16]) ? S0.u[31:16] : S1.u[31:16]; D.u[15:0]  = (S0.u[15:0]  >= S1.u[15:0])  ? S0.u[15:0]  : S1.u[15:0].
13	V_PK_MIN_U16	Packed minimum of unsigned shorts. D.u[31:16] = (S0.u[31:16] < S1.u[31:16]) ? S0.u[31:16] : S1.u[31:16]; D.u[15:0]  = (S0.u[15:0]  < S1.u[15:0])  ? S0.u[15:0]  : S1.u[15:0].
14	V_PK_FMA_F16	Packed fused-multiply-add of FP16 values. D.f[31:16] = S0.f[31:16] * S1.f[31:16] + S2.f[31:16]; D.f[15:0]  = S0.f[15:0]  * S1.f[15:0]  + S2.f[15:0].
15	V_PK_ADD_F16	Packed addition of FP16 values. D.f[31:16] = S0.f[31:16] + S1.f[31:16]; D.f[15:0]  = S0.f[15:0]  + S1.f[15:0].
16	V_PK_MUL_F16	Packed multiply of FP16 values. D.f[31:16] = S0.f[31:16] * S1.f[31:16]; D.f[15:0]  = S0.f[15:0]  * S1.f[15:0].
17	V_PK_MIN_F16	Packed minimum of FP16 values. D.f[31:16] = min(S0.f[31:16], S1.f[31:16]); D.f[15:0]  = min(S0.f[15:0], S1.u[15:0]).
18	V_PK_MAX_F16	Packed maximum of FP16 values. D.f[31:16] = max(S0.f[31:16], S1.f[31:16]); D.f[15:0]  = max(S0.f[15:0], S1.f[15:0]).
19	V_DOT2_F32_F16	Dot product of packed FP16 values. D.f32 = S0.f16[0] * S1.f16[0] + S0.f16[1] * S1.f16[1] + S2.f32.
20	V_DOT2_I32_I16	Dot product of signed shorts. D.i32 = S0.i16[0] * S1.i16[0] + S0.i16[1] * S1.i16[1] + S2.i32.
21	V_DOT2_U32_U16	Dot product of unsigned shorts. D.u32 = S0.u16[0] * S1.u16[0] + S0.u16[1] * S1.u16[1] + S2.u32.
22	V_DOT4_I32_I8	Dot product of signed bytes. D.i32 = S0.i8[0] * S1.i8[0] + S0.i8[1] * S1.i8[1] + S0.i8[2] * S1.i8[2] + S0.i8[3] * S1.i8[3] + S2.i32.
23	V_DOT4_U32_U8	Dot product of unsigned bytes. D.u32 = S0.u8[0] * S1.u8[0] + S0.u8[1] * S1.u8[1] + S0.u8[2] * S1.u8[2] + S0.u8[3] * S1.u8[3] + S2.u32.
24	V_DOT8_I32_I4	Dot product of signed nibbles. D.i32 = S0.i4[0] * S1.i4[0] + S0.i4[1] * S1.i4[1] + S0.i4[2] * S1.i4[2] + S0.i4[3] * S1.i4[3] + S0.i4[4] * S1.i4[4] + S0.i4[5] * S1.i4[5] + S0.i4[6] * S1.i4[6] + S0.i4[7] * S1.i4[7] + S2.i32.
25	V_DOT8_U32_U4	Dot product of unsigned nibbles. D.u32 = S0.u4[0] * S1.u4[0] + S0.u4[1] * S1.u4[1] + S0.u4[2] * S1.u4[2] + S0.u4[3] * S1.u4[3] + S0.u4[4] * S1.u4[4] + S0.u4[5] * S1.u4[5] + S0.u4[6] * S1.u4[6] + S0.u4[7] * S1.u4[7] + S2.u32.
32	V_FMA_MIX_F32	Fused-multiply-add of single-precision values with MIX encoding. Size and location of S0, S1 and S2 controlled by OPSEL: 0=src[31:0], 1=src[31:0], 2=src[15:0], 3=src[31:16].  Also, for MAD_MIX, the NEG_HI field acts instead as an absolute-value modifier. D.f[31:0] = S0.f * S1.f + S2.f.
33	V_FMA_MIXLO_F16	Fused-multiply-add of FP16 values with MIX encoding, result stored in low 16 bits of destination.  Size and location of S0, S1 and S2 controlled by OPSEL: 0=src[31:0], 1=src[31:0], 2=src[15:0], 3=src[31:16].  Also, for MAD_MIX, the NEG_HI field acts instead as an absolute-value modifier. D.f[15:0] = S0.f * S1.f + S2.f.

# VINTERP Instructions
0	V_INTERP_P1_F32	Parameter interpolation, first pass. D.f32 = P10[S1.u32].f32 * S0.f32 + P0[S1.u3].f32. CAUTION: when in HALF_LDS mode, D must not be the same GPR as S; if D == S then data corruption will occur. NOTE: In textual representations the I/J VGPR is the first source and the attribute is the second source; however in the VOP3 encoding the attribute is stored in the src0 field and the VGPR is stored in the src1 field.
1	V_INTERP_P2_F32	Parameter interpolation, second pass. D.f = P20[S1.u] * S0.f + D.f. NOTE: In textual representations the I/J VGPR is the first source and the attribute is the second source; however in the VOP3 encoding the attribute is stored in the src0 field and the VGPR is stored in the src1 field.
2	V_INTERP_MOV_F32	Parameter load. Used for custom interpolation in the shader. D.f = {P10,P20,P0}[S1.u].

# VOP3A & VOP3B Instructions
320	V_FMA_LEGACY_F32	Multiply and add single-precision values.  Follows DX9 rules where 0.0 times anything produces 0.0 (this is not IEEE compliant). D.f = S0.f * S1.f + S2.f. // DX9 rules, 0.0 * x = 0.0
322	V_MAD_I32_I24	Multiply two signed 24-bit integers, add a signed 32-bit integer and store the result as a signed 32-bit integer. This opcode is as efficient as basic single-precision opcodes since it utilizes the single-precision floating point multiplier. D.i = S0.i[23:0] * S1.i[23:0] + S2.i.
323	V_MAD_U32_U24	Multiply two unsigned 24-bit integers, add an unsigned 32-bit integer and store the result as an unsigned 32-bit integer. This opcode is as efficient as basic single-precision opcodes since it utilizes the single-precision floating point multiplier. D.u = S0.u[23:0] * S1.u[23:0] + S2.u.
324	V_CUBEID_F32	Cubemap Face ID determination.  Result is a floating point face ID. // Set D.f = cubemap face ID ({0.0, 1.0, ..., 5.0}). // XYZ coordinate is given in (S0.f, S1.f, S2.f). // S0.f = x // S1.f = y // S2.f = z if (abs(S2.f) >= abs(S0.f) && abs(S2.f) >= abs(S1.f)) if (S2.f < 0) D.f = 5.0; else D.f = 4.0; endif; else if (abs(S1.f) >= abs(S0.f)) if (S1.f < 0) D.f = 3.0; else D.f = 2.0; endif; else if (S0.f < 0) D.f = 1.0; else D.f = 0.0; endif; endif.
325	V_CUBESC_F32	Cubemap S coordinate. // D.f = cubemap S coordinate. // XYZ coordinate is given in (S0.f, S1.f, S2.f). // S0.f = x // S1.f = y // S2.f = z if (abs(S2.f) >= abs(S0.f) && abs(S2.f) >= abs(S1.f)) if (S2.f < 0) D.f = -S0.f; else D.f = S0.f; endif; else if (abs(S1.f) >= abs(S0.f)) D.f = S0.f; else if (S0.f < 0) D.f = S2.f; else D.f = -S2.f; endif; endif.
326	V_CUBETC_F32	Cubemap T coordinate. // D.f = cubemap T coordinate. // XYZ coordinate is given in (S0.f, S1.f, S2.f). // S0.f = x // S1.f = y // S2.f = z if (abs(S2.f) >= abs(S0.f) && abs(S2.f) >= bs(S1.f)) D.f = -S1.f; else if (abs(S1.f) >= abs(S0.f)) if (S1.f < 0) D.f = -S2.f; else D.f = S2.f; endif; else D.f = -S1.f; endif.
327	V_CUBEMA_F32	Determine cubemap major axis. // D.f = 2.0 * cubemap major axis. // XYZ coordinate is given in (S0.f, S1.f, S2.f). // S0.f = x // S1.f = y // S2.f = z if (abs(S2.f) >= abs(S0.f) && abs(S2.f) >= abs(S1.f)) D.f = 2.0 * S2.f; else if (abs(S1.f) >= abs(S0.f)) D.f = 2.0 * S1.f; else D.f = 2.0 * S0.f; endif.
328	V_BFE_U32	Bitfield extract with S0 = data, S1 = field_offset, S2 = field_width. D.u = (S0.u >> S1.u[4:0]) & ((1 << S2.u[4:0]) - 1).
329	V_BFE_I32	Bitfield extract with S0 = data, S1 = field_offset, S2 = field_width. D.i = (S0.i >> S1.u[4:0]) & ((1 << S2.u[4:0]) - 1).
330	V_BFI_B32	Bitfield insert. D.u = (S0.u & S1.u) | (~S0.u & S2.u).
331	V_FMA_F32	Fused single precision multiply add.  0.5ULP accuracy, denormals are supported. D.f = S0.f * S1.f + S2.f.
332	V_FMA_F64	Fused double precision multiply add.  0.5ULP precision, denormals are supported. D.d = S0.d * S1.d + S2.d.
333	V_LERP_U8	Unsigned 8-bit pixel average on packed unsigned bytes (linear interpolation).  S2 acts as a round mode; if set, 0.5 rounds up, otherwise 0.5 truncates. D.u = ((S0.u[31:24] + S1.u[31:24] + S2.u[24]) >> 1) << 24 D.u += ((S0.u[23:16] + S1.u[23:16] + S2.u[16]) >> 1) << 16; D.u += ((S0.u[15:8] + S1.u[15:8] + S2.u[8]) >> 1) << 8; D.u += ((S0.u[7:0] + S1.u[7:0] + S2.u[0]) >> 1).
334	V_ALIGNBIT_B32	Align a value to the specified bit position. D.u = ({S0,S1} >> S2.u[4:0]) & 0xffffffff.
335	V_ALIGNBYTE_B32	Align a value to the specified byte position. D.u = ({S0,S1} >> (8*S2.u[4:0])) & 0xffffffff.
336	V_MULLIT_F32	Multiply for lighting.  Specific rules apply: 0.0 * x = 0.0; Specific INF, NaN, overflow rules. D.f = S0.f * S1.f
337	V_MIN3_F32	Return minimum single-precision value of three inputs. D.f = V_MIN_F32(V_MIN_F32(S0.f, S1.f), S2.f).
338	V_MIN3_I32	Return minimum signed integer value of three inputs. D.i = V_MIN_I32(V_MIN_I32(S0.i, S1.i), S2.i).
339	V_MIN3_U32	Return minimum unsigned integer value of three inputs. D.u = V_MIN_U32(V_MIN_U32(S0.u, S1.u), S2.u).
340	V_MAX3_F32	Return maximum single precision value of three inputs. D.f = V_MAX_F32(V_MAX_F32(S0.f, S1.f), S2.f).
341	V_MAX3_I32	Return maximum signed integer value of three inputs. D.i = V_MAX_I32(V_MAX_I32(S0.i, S1.i), S2.i).
342	V_MAX3_U32	Return maximum unsigned integer value of three inputs. D.u = V_MAX_U32(V_MAX_U32(S0.u, S1.u), S2.u).
343	V_MED3_F32	Return median single precision value of three inputs. if (isNan(S0.f) || isNan(S1.f) || isNan(S2.f)) D.f = V_MIN3_F32(S0.f, S1.f, S2.f); else if (V_MAX3_F32(S0.f, S1.f, S2.f) == S0.f) D.f = V_MAX_F32(S1.f, S2.f); else if (V_MAX3_F32(S0.f, S1.f, S2.f) == S1.f) D.f = V_MAX_F32(S0.f, S2.f); else D.f = V_MAX_F32(S0.f, S1.f); endif.
344	V_MED3_I32	Return median signed integer value of three inputs. if (V_MAX3_I32(S0.i, S1.i, S2.i) == S0.i) D.i = V_MAX_I32(S1.i, S2.i); else if (V_MAX3_I32(S0.i, S1.i, S2.i) == S1.i) D.i = V_MAX_I32(S0.i, S2.i); else D.i = V_MAX_I32(S0.i, S1.i); endif.
345	V_MED3_U32	Return median unsigned integer value of three inputs. if (V_MAX3_U32(S0.u, S1.u, S2.u) == S0.u) D.u = V_MAX_U32(S1.u, S2.u); else if (V_MAX3_U32(S0.u, S1.u, S2.u) == S1.u) D.u = V_MAX_U32(S0.u, S2.u); else D.u = V_MAX_U32(S0.u, S1.u); endif.
346	V_SAD_U8	Sum of absolute differences with accumulation, overflow into upper bits is allowed. ABSDIFF(x, y) := (x > y ? x - y : y - x) // UNSIGNED comparison D.u  = S2.u; D.u += ABSDIFF(S0.u[31:24], S1.u[31:24]); D.u += ABSDIFF(S0.u[23:16], S1.u[23:16]); D.u += ABSDIFF(S0.u[15:8],  S1.u[15:8]); D.u += ABSDIFF(S0.u[7:0],S1.u[7:0]).
347	V_SAD_HI_U8	Sum of absolute differences with accumulation, accumulate into the higher-order bits of S2. D.u = (V_SAD_U8(S0, S1, 0) << 16) + S2.u.
348	V_SAD_U16	Short SAD with accumulation. ABSDIFF(x, y) := (x > y ? x - y : y - x) // UNSIGNED comparison D.u  = S2.u; D.u += ABSDIFF(S0.u[31:16], S1.u[31:16]); D.u += ABSDIFF(S0.u[15:0],  S1.u[15:0]).
349	V_SAD_U32	Dword SAD with accumulation. ABSDIFF(x, y) := (x > y ? x - y : y - x) // UNSIGNED comparison D.u = ABSDIFF(S0.u, S1.u) + S2.u.
350	V_CVT_PK_U8_F32	Convert floating point value S0 to 8-bit unsigned integer and pack the result into byte S1 of dword S2. D.u = (S2.u & ~(0xff << (8 * S1.u[1:0]))); D.u = D.u | ((flt32_to_uint8(S0.f) & 0xff) << (8 * S1.u[1:0])).
352	V_DIV_FIXUP_F64	Double precision division fixup. S0 = Quotient, S1 = Denominator, S2 = Numerator. Given a numerator, denominator, and quotient from a divide, this opcode will detect and apply specific case numerics, touching up the quotient if necessary. This opcode also generates invalid, denorm and divide by zero exceptions caused by the division. sign_out =  sign(S1.d)^sign(S2.d); if (S2.d == NAN) D.d = Quiet(S2.d); else if (S1.d == NAN) D.d = Quiet(S1.d); else if (S1.d == S2.d == 0) // 0/0 D.d = 0xfff8_0000_0000_0000; else if (abs(S1.d) == abs(S2.d) == +-INF) // inf/inf D.d = 0xfff8_0000_0000_0000; else if (S1.d == 0 || abs(S2.d) == +-INF) // x/0, or inf/y D.d = sign_out ? -INF : +INF; else if (abs(S1.d) == +-INF || S2.d == 0) // x/inf, 0/y D.d = sign_out ? -0 : 0; else if ((exponent(S2.d) - exponent(S1.d)) < -1075) D.d = sign_out ? -underflow : underflow; else if (exponent(S1.d) == 2047) D.d = sign_out ? -overflow : overflow; else D.d = sign_out ? -abs(S0.d) : abs(S0.d); endif.
356	V_ADD_F64	Add two double-precision values.  0.5ULP precision, denormals are supported. D.d = S0.d + S1.d.
357	V_MUL_F64	Multiply two double-precision values.  0.5ULP precision, denormals are supported. D.d = S0.d * S1.d.
358	V_MIN_F64	Compute the minimum of two double-precision floats. if (IEEE_MODE && S0.d == sNaN) D.d = Quiet(S0.d); else if (IEEE_MODE && S1.d == sNaN) D.d = Quiet(S1.d); else if (S0.d == NaN) D.d = S1.d; else if (S1.d == NaN) D.d = S0.d; else if (S0.d == +0.0 && S1.d == -0.0) D.d = S1.d; else if (S0.d == -0.0 && S1.d == +0.0) D.d = S0.d; else // Note: there's no IEEE special case here like there is for V_MAX_F64. D.d = (S0.d < S1.d ? S0.d : S1.d); endif.
359	V_MAX_F64	Compute the maximum of two double-precision floats. if (IEEE_MODE && S0.d == sNaN) D.d = Quiet(S0.d); else if (IEEE_MODE && S1.d == sNaN) D.d = Quiet(S1.d); else if (S0.d == NaN) D.d = S1.d; else if (S1.d == NaN) D.d = S0.d; else if (S0.d == +0.0 && S1.d == -0.0) D.d = S0.d; else if (S0.d == -0.0 && S1.d == +0.0) D.d = S1.d; else if (IEEE_MODE) D.d = (S0.d >= S1.d ? S0.d : S1.d); else D.d = (S0.d > S1.d ? S0.d : S1.d); endif.
360	V_LDEXP_F64	Multiply a double-precision float by an integral power of 2, compare with the ldexp() function in C. D.d = S0.d * (2 ** S1.i).
361	V_MUL_LO_U32	Multiply two unsigned integers.  If you only need to multiply integers with small magnitudes consider V_MUL_U32_U24, which is a faster implementation. D.u = S0.u * S1.u.
362	V_MUL_HI_U32	Multiply two unsigned integers and store the high 32 bits of the result.  If you only need to multiply integers with small magnitudes consider V_MUL_HI_U32_U24, which is a faster implementation. D.u = (S0.u * S1.u) >> 32.
364	V_MUL_HI_I32	Multiply two signed integers and store the high 32 bits of the result.  If you only need to multiply integers with small magnitudes consider V_MUL_HI_I32_I24, which is a faster implementation. D.i = (S0.i * S1.i) >> 32.
365	V_DIV_SCALE_F32	Single precision division pre-scale.  S0 = Input to scale (either denominator or numerator), S1 = Denominator, S2 = Numerator. Given a numerator and denominator, this opcode will appropriately scale inputs for division to avoid subnormal terms during Newton-Raphson correction algorithm.  S0 must be the same value as either S1 or S2. This opcode producses a VCC flag for post-scaling of the quotient (using V_DIV_FMAS_F32). VCC = 0; if (S2.f == 0 || S1.f == 0) D.f = NAN else if (exponent(S2.f) - exponent(S1.f) >= 96) // N/D near MAX_FLOAT VCC = 1; if (S0.f == S1.f) // Only scale the denominator D.f = ldexp(S0.f, 64); end if else if (S1.f == DENORM) D.f = ldexp(S0.f, 64); else if (1 / S1.f == DENORM && S2.f / S1.f == DENORM) VCC = 1; if (S0.f == S1.f) // Only scale the denominator D.f = ldexp(S0.f, 64); end if else if (1 / S1.f == DENORM) D.f = ldexp(S0.f, -64); else if (S2.f / S1.f==DENORM) VCC = 1; if (S0.f == S2.f) // Only scale the numerator D.f = ldexp(S0.f, 64); end if else if (exponent(S2.f) <= 23) // Numerator is tiny D.f = ldexp(S0.f, 64); end if.
366	V_DIV_SCALE_F64	Double precision division pre-scale.  S0 = Input to scale (either denominator or numerator), S1 = Denominator, S2 = Numerator. Given a numerator and denominator, this opcode will appropriately scale inputs for division to avoid subnormal terms during Newton-Raphson correction algorithm.  S0 must be the same value as either S1 or S2. This opcode producses a VCC flag for post-scaling of the quotient (using V_DIV_FMAS_F64). VCC = 0; if (S2.d == 0 || S1.d == 0) D.d = NAN else if (exponent(S2.d) - exponent(S1.d) >= 768) // N/D near MAX_FLOAT VCC = 1; if (S0.d == S1.d) // Only scale the denominator D.d = ldexp(S0.d, 128); end if else if (S1.d == DENORM) D.d = ldexp(S0.d, 128); else if (1 / S1.d == DENORM && S2.d / S1.d == DENORM) VCC = 1; if (S0.d == S1.d) // Only scale the denominator D.d = ldexp(S0.d, 128); end if else if (1 / S1.d == DENORM) D.d = ldexp(S0.d, -128); else if (S2.d / S1.d==DENORM) VCC = 1; if (S0.d == S2.d) // Only scale the numerator D.d = ldexp(S0.d, 128); end if else if (exponent(S2.d) <= 53) // Numerator is tiny D.d = ldexp(S0.d, 128); end if.
367	V_DIV_FMAS_F32	Single precision FMA with fused scale. This opcode performs a standard Fused Multiply-Add operation and will conditionally scale the resulting exponent if VCC is set. Input denormals are not flushed, but output flushing is allowed. if (VCC[threadId]) D.f = 2**32 * (S0.f * S1.f + S2.f); else D.f = S0.f * S1.f + S2.f; end if.
368	V_DIV_FMAS_F64	Double precision FMA with fused scale. This opcode performs a standard Fused Multiply-Add operation and will conditionally scale the resulting exponent if VCC is set. Input denormals are not flushed, but output flushing is allowed. if (VCC[threadId]) D.d = 2**64 * (S0.d * S1.d + S2.d); else D.d = S0.d * S1.d + S2.d; end if.
369	V_MSAD_U8	Masked sum of absolute differences with accumulation, overflow into upper bits is allowed.  Components where the reference value in S1 is zero are not included in the sum. ABSDIFF(x, y) := (x > y ? x - y : y - x) // UNSIGNED comparison D.u  = S2.u; D.u += S1.u[31:24] == 0 ? 0 : ABSDIFF(S0.u[31:24], S1.u[31:24]); D.u += S1.u[23:16] == 0 ? 0 : ABSDIFF(S0.u[23:16], S1.u[23:16]); D.u += S1.u[15:8]  == 0 ? 0 : ABSDIFF(S0.u[15:8],  S1.u[15:8]); D.u += S1.u[7:0]== 0 ? 0 : ABSDIFF(S0.u[7:0],S1.u[7:0]).
370	V_QSAD_PK_U16_U8	Quad-byte SAD with 16-bit packed accumulation. D[63:48] = SAD_U8(S0[55:24], S1[31:0], S2[63:48]); D[47:32] = SAD_U8(S0[47:16], S1[31:0], S2[47:32]); D[31:16] = SAD_U8(S0[39:8],  S1[31:0], S2[31:16]); D[15:0]  = SAD_U8(S0[31:0],  S1[31:0], S2[15:0]).
371	V_MQSAD_PK_U16_U8	Quad-byte masked SAD with 16-bit packed accumulation. D[63:48] = MSAD_U8(S0[55:24], S1[31:0], S2[63:48]); D[47:32] = MSAD_U8(S0[47:16], S1[31:0], S2[47:32]); D[31:16] = MSAD_U8(S0[39:8],  S1[31:0], S2[31:16]); D[15:0]  = MSAD_U8(S0[31:0],  S1[31:0], S2[15:0]).
372	V_TRIG_PREOP_F64	Look Up 2/PI (S0.d) with segment select S1.u[4:0].  This operation returns an aligned, double precision segment of 2/PI needed to do range reduction on S0.d (double-precision value). Multiple segments can be specified through S1.u[4:0].  Rounding is round-to-zero.  Large inputs (exp > 1968) are scaled to avoid loss of precision through denormalization. shift = S1.u * 53; if exponent(S0.d) > 1077 then shift += exponent(S0.d) - 1077; endif result = (double) ((2/PI[1200:0] << shift) & 0x1fffff_ffffffff); scale = (-53 - shift); if exponent(S0.d) >= 1968 then scale += 128; endif D.d = ldexp(result, scale).
373	V_MQSAD_U32_U8	Quad-byte masked SAD with 32-bit packed accumulation. D[127:96] = MSAD_U8(S0[55:24], S1[31:0], S2[127:96]); D[95:64]  = MSAD_U8(S0[47:16], S1[31:0], S2[95:64]); D[63:32]  = MSAD_U8(S0[39:8],  S1[31:0], S2[63:32]); D[31:0]= MSAD_U8(S0[31:0],  S1[31:0], S2[31:0]).
374	V_MAD_U64_U32	Multiply and add unsigned integers and produce a 64-bit result. {vcc_out,D.u64} = S0.u32 * S1.u32 + S2.u64.
375	V_MAD_I64_I32	Multiply and add signed integers and produce a 64-bit result. {vcc_out,D.i64} = S0.i32 * S1.i32 + S2.i64.
376	V_XOR3_B32	Bitwise XOR of three inputs.  Input and output modifiers not supported. D.u32 = S0.u32 ^ S1.u32 ^ S2.u32.
767	V_LSHLREV_B64	Logical shift left, count is in the first operand.  Only one scalar broadcast constant is allowed. D.u64 = S1.u64 << S0.u[5:0].
768	V_LSHRREV_B64	Logical shift right, count is in the first operand.  Only one scalar broadcast constant is allowed. D.u64 = S1.u64 >> S0.u[5:0].
769	V_ASHRREV_I64	Arithmetic shift right (preserve sign bit), count is in the first operand.  Only one scalar broadcast constant is allowed. D.u64 = signext(S1.u64) >> S0.u[5:0].
771	V_ADD_NC_U16	Add two unsigned shorts.  Supports saturation (unsigned 16-bit integer domain).  No carry-in or carry-out. D.u16 = S0.u16 + S1.u16.
772	V_SUB_NC_U16	Subtract the second unsigned short from the first.  Supports saturation (unsigned 16-bit integer domain).  No carry-in or carry-out. D.u16 = S0.u16 - S1.u16.
773	V_MUL_LO_U16	Multiply two unsigned shorts.  Supports saturation (unsigned 16-bit integer domain). D.u16 = S0.u16 * S1.u16.
775	V_LSHRREV_B16	Logical shift right, count is in the first operand. D.u[15:0] = S1.u[15:0] >> S0.u[3:0].
776	V_ASHRREV_I16	Arithmetic shift right (preserve sign bit), count is in the first operand. D.i[15:0] = signext(S1.i[15:0]) >> S0.i[3:0].
777	V_MAX_U16	Maximum of two unsigned shorts. D.u16 = (S0.u16 >= S1.u16 ? S0.u16 : S1.u16).
778	V_MAX_I16	Maximum of two signed shorts. D.i16 = (S0.i16 >= S1.i16 ? S0.i16 : S1.i16).
779	V_MIN_U16	Minimum of two unsigned shorts. D.u16 = (S0.u16 < S1.u16 ? S0.u16 : S1.u16).
780	V_MIN_I16	Minimum of two signed shorts. D.i16 = (S0.i16 < S1.i16 ? S0.i16 : S1.i16).
781	V_ADD_NC_I16	Add two signed shorts.  Supports saturation (signed 16-bit integer domain).  No carry-in or carry-out. D.i16 = S0.i16 + S1.i16.
782	V_SUB_NC_I16	Subtract the second signed short from the first.  Supports saturation (unsigned 16-bit integer domain).  No carry-in or carry-out. D.i16 = S0.i16 - S1.i16.
783	V_ADD_CO_U32	Add two unsigned integers with carry-out.  In VOP3 the VCC destination may be an arbitrary SGPR-pair. D.u32 = S0.u32 + S1.u32; VCC = S0.u + S1.u >= 0x100000000ULL ? 1 : 0.
784	V_SUB_CO_U32	Subtract the second unsigned integer from the first with carry-out.  In VOP3 the VCC destination may be an arbitrary SGPR-pair. D.u = S0.u - S1.u; VCC[threadId] = (S1.u > S0.u ? 1 : 0). // VCC is an UNSIGNED overflow/carry-out for V_SUB_CO_CI_U32.
785	V_PACK_B32_F16	Pack two FP16 values together. D[31:16].f16 = S1.f16; D[15:0].f16 = S0.f16.
786	V_CVT_PKNORM_I16_F16	Convert two FP16 values into packed signed normalized shorts. D = {(snorm)S1.f16, (snorm)S0.f16}.
787	V_CVT_PKNORM_U16_F16	Convert two FP16 values into packed unsigned normalized shorts. D = {(unorm)S1.f16, (unorm)S0.f16}.
788	V_LSHLREV_B16	Logical shift left, count is in the first operand. D.u[15:0] = S1.u[15:0] << S0.u[3:0].
793	V_SUBREV_CO_U32	Subtract the first unsigned integer from the second with carry-out.  In VOP3 the VCC destination may be an arbitrary SGPR-pair. D.u = S1.u - S0.u; VCC[threadId] = (S0.u > S1.u ? 1 : 0). // VCC is an UNSIGNED overflow/carry-out for V_SUB_CO_CI_U32.
832	V_MAD_U16	Multiply and add unsigned shorts.  Supports saturation (unsigned 16-bit integer domain). If op_sel[3] is 0: Result is written to 16 LSBs of destination VGPR and hi 16 bits are preserved. If op_sel[3] is 1: Result is written to 16 MSBs of destination VGPR and lo 16 bits are preserved. D.u16 = S0.u16 * S1.u16 + S2.u16.
834	V_INTERP_P1LL_F16	FP16 parameter interpolation.  `LL' stands for `two LDS arguments'.  attr_word selects the high or low half 16 bits of each LDS dword accessed.  This opcode is available for 32-bank LDS only. NOTE: In textual representations the I/J VGPR is the first source and the attribute is the second source; however in the VOP3 encoding the attribute is stored in the src0 field and the VGPR is stored in the src1 field. D.f32 = P10.f16 * S0.f32 + P0.f16.
835	V_INTERP_P1LV_F16	FP16 parameter interpolation.  `LV' stands for `One LDS and one VGPR argument'.  S2 holds two parameters, attr_word selects the high or low word of the VGPR for this calculation, as well as the high or low half of the LDS data.  Meant for use with 16-bank LDS. NOTE: In textual representations the I/J VGPR is the first source and the attribute is the second source; however in the VOP3 encoding the attribute is stored in the src0 field and the VGPR is stored in the src1 field. D.f32 = P10.f16 * S0.f32 + (S2.u32 >> (attr_word * 16)).f16.
836	V_PERM_B32	Byte permute. D.u[31:24] = byte_permute({S0.u, S1.u}, S2.u[31:24]); D.u[23:16] = byte_permute({S0.u, S1.u}, S2.u[23:16]); D.u[15:8] = byte_permute({S0.u, S1.u}, S2.u[15:8]); D.u[7:0] = byte_permute({S0.u, S1.u}, S2.u[7:0]); byte permute(byte in[8], byte sel) { if(sel>=13) then return 0xff; elsif(sel==12) then return 0x00; elsif(sel==11) then return in[7][7] * 0xff; elsif(sel==10) then return in[5][7] * 0xff; elsif(sel==9) then return in[3][7] * 0xff; elsif(sel==8) then return in[1][7] * 0xff; else return in[sel]; }
837	V_XAD_U32	Bitwise XOR and then add.  No carryin/carryout and no saturation.  This opcode exists to accelerate the SHA256 hash algorithm. D.u32 = (S0.u32 ^ S1.u32) + S2.u32.
838	V_LSHL_ADD_U32	Logical shift left and then add. D.u = (S0.u << S1.u[4:0]) + S2.u.
839	V_ADD_LSHL_U32	Add and then logical shift left the result. D.u = (S0.u + S1.u) << S2.u[4:0].
843	V_FMA_F16	Fused half precision multiply add of FP16 values.  0.5ULP accuracy, denormals are supported. If op_sel[3] is 0 Result is written to 16 LSBs of destination VGPR and hi 16 bits are preserved. If op_sel[3] is 1 Result is written to 16 MSBs of destination VGPR and lo 16 bits are preserved. D.f16 = S0.f16 * S1.f16 + S2.f16.
849	V_MIN3_F16	Return minimum FP16 value of three inputs. D.f16 = V_MIN_F16(V_MIN_F16(S0.f16, S1.f16), S2.f16).
850	V_MIN3_I16	Return minimum signed short value of three inputs. D.i16 = V_MIN_I16(V_MIN_I16(S0.i16, S1.i16), S2.i16).
851	V_MIN3_U16	Return minimum unsigned short value of three inputs. D.u16 = V_MIN_U16(V_MIN_U16(S0.u16, S1.u16), S2.u16).
852	V_MAX3_F16	Return maximum FP16 value of three inputs. D.f16 = V_MAX_F16(V_MAX_F16(S0.f16, S1.f16), S2.f16).
853	V_MAX3_I16	Return maximum signed short value of three inputs. D.i16 = V_MAX_I16(V_MAX_I16(S0.i16, S1.i16), S2.i16).
854	V_MAX3_U16	Return maximum unsigned short value of three inputs. D.u16 = V_MAX_U16(V_MAX_U16(S0.u16, S1.u16), S2.u16).
855	V_MED3_F16	Return median FP16 value of three inputs. if (isNan(S0.f16) || isNan(S1.f16) || isNan(S2.f16)) D.f16 = V_MIN3_F16(S0.f16, S1.f16, S2.f16); else if (V_MAX3_F16(S0.f16, S1.f16, S2.f16) == S0.f16) D.f16 = V_MAX_F16(S1.f16, S2.f16); else if (V_MAX3_F16(S0.f16, S1.f16, S2.f16) == S1.f16) D.f16 = V_MAX_F16(S0.f16, S2.f16); else D.f16 = V_MAX_F16(S0.f16, S1.f16); endif.
856	V_MED3_I16	Return median signed short value of three inputs. if (V_MAX3_I16(S0.i16, S1.i16, S2.i16) == S0.i16) D.i16 = V_MAX_I16(S1.i16, S2.i16); else if (V_MAX3_I16(S0.i16, S1.i16, S2.i16) == S1.i16) D.i16 = V_MAX_I16(S0.i16, S2.i16); else D.i16 = V_MAX_I16(S0.i16, S1.i16); endif.
857	V_MED3_U16	Return median unsigned short value of three inputs. if (V_MAX3_U16(S0.u16, S1.u16, S2.u16) == S0.u16) D.u16 = V_MAX_U16(S1.u16, S2.u16); else if (V_MAX3_U16(S0.u16, S1.u16, S2.u16) == S1.u16) D.u16 = V_MAX_U16(S0.u16, S2.u16); else D.u16 = V_MAX_U16(S0.u16, S1.u16); endif.
858	V_INTERP_P2_F16	FP16 parameter interpolation.  Final computation.  attr_word selects LDS high or low 16bits.  Used for both 16- and 32-bank LDS. NOTE: In textual representations the I/J VGPR is the first source and the attribute is the second source; however in the VOP3 encoding the attribute is stored in the src0 field and the VGPR is stored in the src1 field. If op_sel[3] is 0 Result is written to 16 LSBs of destination VGPR and hi 16 bits are preserved. If op_sel[3] is 1 Result is written to 16 MSBs of destination VGPR and lo 16 bits are preserved. D.f16 = P20.f16 * S0.f32 + S2.f32.
862	V_MAD_I16	Multiply and add signed short values.  Supports saturation (signed 16-bit integer domain). If op_sel[3] is 0 Result is written to 16 LSBs of destination VGPR and hi 16 bits are preserved. If op_sel[3] is 1 Result is written to 16 MSBs of destination VGPR and lo 16 bits are preserved. D.i16 = S0.i16 * S1.i16 + S2.i16.
863	V_DIV_FIXUP_F16	Half precision division fixup. S0 = Quotient, S1 = Denominator, S2 = Numerator. Given a numerator, denominator, and quotient from a divide, this opcode will detect and apply specific case numerics, touching up the quotient if necessary. This opcode also generates invalid, denorm and divide by zero exceptions caused by the division. If op_sel[3] is 0 Result is written to 16 LSBs of destination VGPR and hi 16 bits are preserved. If op_sel[3] is 1 Result is written to 16 MSBs of destination VGPR and lo 16 bits are preserved. sign_out =  sign(S1.f16)^sign(S2.f16); if (S2.f16 == NAN) D.f16 = Quiet(S2.f16); else if (S1.f16 == NAN) D.f16 = Quiet(S1.f16); else if (S1.f16 == S2.f16 == 0) // 0/0 D.f16 = 0xfe00; else if (abs(S1.f16) == abs(S2.f16) == +-INF) // inf/inf D.f16 = 0xfe00; else if (S1.f16 ==0 || abs(S2.f16) == +-INF) // x/0, or inf/y D.f16 = sign_out ? -INF : +INF; else if (abs(S1.f16) == +-INF || S2.f16 == 0) // x/inf, 0/y D.f16 = sign_out ? -0 : 0; else D.f16 = sign_out ? -abs(S0.f16) : abs(S0.f16); end if.
864	V_READLANE_B32	Copy one VGPR value to one SGPR.  D = SGPR-dest, S0 = Source Data (VGPR# or M0(lds-direct)), S1 = Lane Select (SGPR or M0). Lane is S1 % (32 if wave32, 64 if wave64).  Ignores exec mask. Input and output modifiers not supported; this is an untyped operation. if(wave32) SMEM[D_ADDR] = VMEM[S0_ADDR][S1[4:0]]; // For wave32 else SMEM[D_ADDR] = VMEM[S0_ADDR][S1[5:0]]; // For wave64 endif.
865	V_WRITELANE_B32	Write value into one VGPR in one lane.  D = VGPR-dest, S0 = Source Data (sgpr, m0, exec or constants), S1 = Lane Select (SGPR or M0).  Lane is S1 % (32 if wave32, 64 if wave64). Ignores exec mask. Input and output modifiers not supported; this is an untyped operation. if(wave32) VMEM[D_ADDR][S1[4:0]] = SMEM[S0_ADDR]; // For wave32 else VMEM[D_ADDR][S1[5:0]] = SMEM[S0_ADDR]; // For wave64 endif.
866	V_LDEXP_F32	Multiply a single-precision float by an integral power of 2, compare with the ldexp() function in C. D.f = S0.f * (2 ** S1.i).
867	V_BFM_B32	Bitfield modify. S0 is the bitfield width and S1 is the bitfield offset. D.u32 = ((1<<S0[4:0])-1) << S1[4:0].
868	V_BCNT_U32_B32	Bit count. D.u = S1.u; for i in 0 .. 31 do D.u += S0.u[i]; // count i'th bit endfor.
869	V_MBCNT_LO_U32_B	Masked bit count, ThreadPosition is the position of this thread 32 in the wavefront (in 0..63).  See also V_MBCNT_HI_U32_B32. ThreadMask = (1LL << ThreadPosition) - 1; MaskedValue = (S0.u & ThreadMask[31:0]); D.u = S1.u; for i in 0 ... 31 do D.u += (MaskedValue[i] == 1 ? 1 : 0); endfor.
870	V_MBCNT_HI_U32_B	Masked bit count, ThreadPosition is the position of this thread 32 in the wavefront (in 0..63).  See also V_MBCNT_LO_U32_B32.  Note that in Wave32 mode ThreadMask[63:32] == 0 and this instruction simply performs a move from S1 to D. ThreadMask = (1LL << ThreadPosition) - 1; MaskedValue = (S0.u & ThreadMask[63:32]); D.u = S1.u; for i in 0 ... 31 do D.u += (MaskedValue[i] == 1 ? 1 : 0); endfor. Example to compute each thread's position in 0..63: v_mbcnt_lo_u32_b32 v0, -1, 0 v_mbcnt_hi_u32_b32 v0, -1, v0 // v0 now contains ThreadPosition
872	V_CVT_PKNORM_I16_F32	Convert two single-precision floats into a packed signed normalized value. D.i16_lo = (snorm)S0.f32; D.i16_hi = (snorm)S1.f32.
873	V_CVT_PKNORM_U16_F32	Convert two single-precision floats into a packed unsigned normalized value. D.u16_lo = (unorm)S0.f32; D.u16_hi = (unorm)S1.f32.
874	V_CVT_PK_U16_U32	Convert two unsigned integers into a packed unsigned short. D.u16_lo = u32_to_u16(S0.u32); D.u16_hi = u32_to_u16(S1.u32).
875	V_CVT_PK_I16_I32	Convert two signed integers into a packed signed short. D.i16_lo = i32_to_i16(S0.i32); D.i16_hi = i32_to_i16(S1.i32).
877	V_ADD3_U32	Add three unsigned integers. D.u = S0.u + S1.u + S2.u.
879	V_LSHL_OR_B32	Logical shift left and then bitwise OR. D.u = (S0.u << S1.u[4:0]) | S2.u.
881	V_AND_OR_B32	Bitwise AND and then bitwise OR. D.u = (S0.u & S1.u) | S2.u.
882	V_OR3_B32	Bitwise OR of three inputs. D.u = S0.u | S1.u | S2.u.
883	V_MAD_U32_U16	Multiply and add unsigned values. D.u32 = S0.u16 * S1.u16 + S2.u32.
885	V_MAD_I32_I16	Multiply and add signed values. D.i32 = S0.i16 * S1.i16 + S2.i32.
886	V_SUB_NC_I32	Subtract the second signed integer from the first.  No carry-in or carry-out.  Supports saturation (signed 32-bit integer domain). D.i = S0.i - S1.i.
888	V_PERMLANEX16_B3	Perform arbitrary gather-style operation across two rows (each 2 row is 16 contiguous lanes). The first source must be a VGPR and the second and third sources must be scalar values; the second and third source are combined into a single 64-bit value representing lane selects used to swizzle within each row. OP_SEL is not used in its typical manner for this instruction. For this instruction OP_SEL[0] is overloaded to represent the DPP 'FI' (Fetch Inactive) bit and OP_SEL[1] is overloaded to represent the DPP 'BOUND_CTRL' bit. The remainin OP_SEL bits are reserved for this instruction. ABS, NEG and OMOD modifiers should all be zeroed for this instruction. Compare with V_PERMLANE16_B32. lanesel = { S2.u, S1.u }; // Concatenate lane select bits for row in 0 ... 3 do // interval is 0 ... 1 for wave32 mode // Implement arbitrary swizzle across two rows altrow = {row[1], ~row[0]}; // 1<->0, 3<->2 for i in 0 ... 15 do D.lane[row * 16 + i] = S0.lane[altrow * 16 + lanesel[i * 4 + 3:i * 4]]; endfor; endfor. Example implementing a rotation across an entire wave32 wavefront: // Note for this to work, source and destination VGPRs must be different. // For this rotation, lane 15 gets data from lane 16, lane 31 gets data from lane 0. // These are the only two lanes that need to use v_permlanex16_b32. v_mov_b32 exec_lo, 0x7fff7fff; // Lanes getting data from their own row v_mov_b32 s0, 0x87654321; v_mov_b32 s1, 0x0fedcba9; v_permlane16_b32 v1, v0, s0, s1 fi; // FI bit needed for lanes 14 and 30 v_mov_b32 exec_lo, 0x80008000; // Lanes getting data from the other row v_permlanex16_b32 v1, v0, s0, s1 fi; // FI bit needed for lanes 15 and 31
895	V_ADD_NC_I32	Add two signed integers.  No carry-in or carry-out.  Supports saturation (signed 32-bit integer domain).  No carry-in or carry-out. D.i = S0.i + S1.i.

#  LDS & GDS Instructions
0	DS_ADD_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
1	DS_SUB_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
2	DS_RSUB_U32	Subtraction with reversed operands. // 32bit addr = VGPR[ADDR]+{INST1,INST0}; tmp = DS[addr].u32; DS[addr].u32 = VGPR[DATA0].u32-DS[addr].u32; VGPR[VDST].u32 = tmp.
3	DS_INC_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
4	DS_DEC_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
5	DS_MIN_I32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
6	DS_MAX_I32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
7	DS_MIN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
8	DS_MAX_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
9	DS_AND_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
10	DS_OR_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
11	DS_XOR_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
12	DS_MSKOR_B32	Masked dword OR, D0 contains the mask and D1 contains the new value. // 32bit tmp = MEM[ADDR]; MEM[ADDR] = (MEM[ADDR] & ~DATA) | DATA2; RETURN_DATA = tmp.
13	DS_WRITE_B32	Write dword. // 32bit MEM[ADDR] = DATA.
14	DS_WRITE2_B32	Write 2 dwords. // 32bit MEM[ADDR + OFFSET0 * 4] = DATA; MEM[ADDR + OFFSET1 * 4] = DATA2.
15	DS_WRITE2ST64_B32	Write 2 dwords with larger stride. // 32bit MEM[ADDR + OFFSET0 * 4 * 64] = DATA; MEM[ADDR + OFFSET1 * 4 * 64] = DATA2.
16	DS_CMPST_B32	Compare and store.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_CMPSWAP opcode. // 32bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
17	DS_CMPST_F32	Floating point compare and store that handles NaN/INF/denormal values.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_FCMPSWAP opcode. // 32bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
18	DS_MIN_F32	Floating point minimum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMIN. // 32bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (cmp < tmp) ? src : tmp.
19	DS_MAX_F32	Floating point maximum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMAX. // 32bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (tmp > cmp) ? src : tmp.
20	DS_NOP	Do nothing.
21	DS_ADD_F32	Floating point add that handles NaN/INF/denormal values. float tmp = MEM[ADDR].f32; MEM[ADDR].f32 += DATA0.f32; VDST.f32 = tmp;
24	DS_GWS_SEMA_RELEASE_ALL	 GDS Only: The GWS resource (rid) indicated will process this opcode by updating the counter and labeling the specified resource as a semaphore. // Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + offset0[5:0]; // Incr the state counter of the resource state.counter[rid] = state.wave_in_queue; state.type = SEMAPHORE; return rd_done; //release calling wave This action will release ALL queued waves; it Will have no effect if no waves are present.
25	DS_GWS_INIT	GDS Only: Initialize a barrier or semaphore resource. // Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + offset0[5:0]; // Get the value to use in init index = find_first_valid(vector mask) value = DATA[thread: index] // Set the state of the resource state.counter[rid] = lsb(value); //limit #waves state.flag[rid] = 0; return rd_done; //release calling wave
26	DS_GWS_SEMA_V	GDS Only: The GWS resource indicated will process this opcode by updating the counter and labeling the resource as a semaphore. //Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + offset0[5:0]; //Incr the state counter of the resource state.counter[rid] += 1; state.type = SEMAPHORE; return rd_done; //release calling wave This action will release one waved if any are queued in this resource.
27	DS_GWS_SEMA_BR	GDS Only: The GWS resource indicated will process this opcode by updating the counter by the bulk release delivered count and labeling the resource as a semaphore. //Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + offset0[5:0]; index =  find first valid (vector mask) count = DATA[thread: index]; //Add count to the resource state counter state.counter[rid] += count; state.type = SEMAPHORE; return rd_done; //release calling wave This action will release count number of waves, promptly if queued, or as they arrive from the noted resource.
28	DS_GWS_SEMA_P	GDS Only: The GWS resource indicated will process this opcode by queueing it until counter enables a release and then decrementing the counter of the resource as a semaphore. //Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + offset0[5:0]; state.type = SEMAPHORE; ENQUEUE until(state[rid].counter > 0) state[rid].counter -= 1; return rd_done;
29	DS_GWS_BARRIER	GDS Only: The GWS resource indicated will process this opcode by queueing it until barrier is satisfied. The number of waves needed is passed in as DATA of first valid thread. //Determine the GWS resource to work on rid[5:0] = gds_base[5:0] + OFFSET0[5:0]; index =  find first valid (vector mask); value = DATA[thread: index]; // Input Decision Machine state.type[rid] = BARRIER; if(state[rid].counter <= 0) then thread[rid].flag = state[rid].flag; ENQUEUE; state[rid].flag = !state.flag; state[rid].counter = value; return rd_done; else state[rid].counter -= 1; thread.flag = state[rid].flag; ENQUEUE; endif. Since the waves deliver the count for the next barrier, this function can have a different size barrier for each occurrence. // Release Machine if(state.type == BARRIER) then if(state.flag != thread.flag) then return rd_done; endif; endif.
30	DS_WRITE_B8	Byte write. MEM[ADDR] = DATA[7:0].
31	DS_WRITE_B16	Short write. MEM[ADDR] = DATA[15:0].
32	DS_ADD_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
33	DS_SUB_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
34	DS_RSUB_RTN_U32	Subtraction with reversed operands. // 32bit addr = VGPR[ADDR]+{INST1,INST0}; tmp = DS[addr].u32; DS[addr].u32 = VGPR[DATA0].u32-DS[addr].u32; VGPR[VDST].u32 = tmp.
35	DS_INC_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
36	DS_DEC_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
37	DS_MIN_RTN_I32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
38	DS_MAX_RTN_I32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
39	DS_MIN_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
40	DS_MAX_RTN_U32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
41	DS_AND_RTN_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
42	DS_OR_RTN_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
43	DS_XOR_RTN_B32	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
44	DS_MSKOR_RTN_B32	Masked dword OR, D0 contains the mask and D1 contains the new value. // 32bit tmp = MEM[ADDR]; MEM[ADDR] = (MEM[ADDR] & ~DATA) | DATA2; RETURN_DATA = tmp.
45	DS_WRXCHG_RTN_B32	Write-exchange operation. tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
46	DS_WRXCHG2_RTN_B32	 Write-exchange 2 separate dwords.
47	DS_WRXCHG2ST64_RTN_B32	 Write-exchange 2 separate dwords with a stride of 64 dwords.
48	DS_CMPST_RTN_B32	Compare and store.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_CMPSWAP opcode. // 32bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
49	DS_CMPST_RTN_F32	Floating point compare and store that handles NaN/INF/denormal values.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_FCMPSWAP opcode. // 32bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
50	DS_MIN_RTN_F32	Floating point minimum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMIN. // 32bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (cmp < tmp) ? src : tmp.
51	DS_MAX_RTN_F32	Floating point maximum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMAX. // 32bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (tmp > cmp) ? src : tmp.
52	DS_WRAP_RTN_B32	tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? tmp - DATA : tmp + DATA2; RETURN_DATA = tmp.
53	DS_SWIZZLE_B32	Dword swizzle, no data is written to LDS memory. See next section for details.
54	DS_READ_B32	Dword read. RETURN_DATA = MEM[ADDR].
55	DS_READ2_B32	Read 2 dwords. RETURN_DATA[0] = MEM[ADDR + OFFSET0 * 4]; RETURN_DATA[1] = MEM[ADDR + OFFSET1 * 4].
56	DS_READ2ST64_B32	Read 2 dwords with a larger stride. RETURN_DATA[0] = MEM[ADDR + OFFSET0 * 4 * 64]; RETURN_DATA[1] = MEM[ADDR + OFFSET1 * 4 * 64].
57	DS_READ_I8	Signed byte read. RETURN_DATA = signext(MEM[ADDR][7:0]).
58	DS_READ_U8	Unsigned byte read. RETURN_DATA = {24'h0,MEM[ADDR][7:0]}.
59	DS_READ_I16	Signed short read. RETURN_DATA = signext(MEM[ADDR][15:0]).
60	DS_READ_U16	Unsigned short read. RETURN_DATA = {16'h0,MEM[ADDR][15:0]}.
61	DS_CONSUME	LDS & GDS. Subtract (count_bits(exec_mask)) from the value stored in DS memory at (M0.base + instr_offset). Return the pre-operation value to VGPRs. The DS will subtract count_bits(vector valid mask) from the value stored at address M0.base + instruction based offset and return the pre-op value to all valid lanes.  This op can be used in both the LDS and GDS.  In the LDS this address will be an offset to HWBASE and clamped by M0.size, but in the GDS the M0.base constant will have the physical GDS address and the compiler must force offset to zero.  In GDS it is for the traditional append buffer operations.  In LDS it is for local thread group appends and can be used to regroup divergent threads.  The use of the M0 register enables the compiler to do indexing of UAV append/consume counters. For GDS (system wide) consume, the compiler must use a zero for {offset1,offset0}, for LDS the compiler will use {offset1,offset0} to provide the relative address to the append counter in the LDS for runtime index offset or index. Inside DS --- Do one atomic add for first valid lane and broadcast result to all valid lanes.  Offset = 0ffset1:offset0; Interpreted as byte offset --- For 10xx LDS designs only aligned atomics are supported, so 2 lsbs of offset must be set to zero. addr = M0.base + offset; // offset by LDS HWBASE, limit to M.size rtnval =  LDS(addr); LDS(addr) = LDS(addr) - countbits(valid mask); GPR[VDST] = rtnval; // return to all valid threads
62	DS_APPEND	LDS & GDS. Add (count_bits(exec_mask)) to the value stored in DS memory at (M0.base + instr_offset). Return the pre-operation value to VGPRs. The DS will add count_bits(vector valid mask) from the value stored at address M0.base + instruction based offset and return the pre-op value to all valid lanes.  This op can be used in both the LDS and GDS.  In the LDS this address will be an offset to HWBASE and clamped by M0.size, but in the GDS the M0.base constant will have the physical GDS address and the compiler must set offset to zero.  In GDS it is for the traditional append buffer operations.  In LDS it is for local thread group appends and can be used to regroup divergent threads.  The use of the M0 register enables the compiler to do indexing of UAV append/consume counters. For GDS (system wide) consume, the compiler must use a zero for {offset1,offset0}, for LDS the compiler will use {offset1,offset0} to provide the relative address to the append counter in the LDS for runtime index offset or index. Inside DS --- Do one atomic add for first valid lane and broadcast result to all valid lanes.  Offset = 0ffset1:offset0; Interpreted as byte offset --- For 10xx LDS designs only aligned atomics will be supported, so 2 lsbs of offset must be set to zero. addr = M0.base + offset; // offset by LDS HWBASE, limit to M.size rtnval =  LDS(addr); LDS(addr) = LDS(addr) + countbits(valid mask); GPR[VDST] = rtnval; // return to all valid threads
63	DS_ORDERED_COUNT	GDS-only.  Add (count_bits(exec_mask)) to one of 4 dedicated ordered-count counters (aka 'packers').  Additional bits of instr.offset field are overloaded to hold packer-id, 'last'. GDS Only: Intercepted by GDS and processed by ordered append module.  The ordered append module will queue request until this request wave is the oldest in the queue at which time the oldest wave request will be dispatched to the DS with an atomic add for execution and broadcast back to ALL lanes of a wave.  This is an ordered count operation and can only be called once per issue with the release flag set.  If the release flag is not set, the wave will have full control over the order count module until it sends a request with the release flag set. Unlike append/consume this op needs to be sent even if there are no valid lanes when it is issued.  The GDS will add zero and advance the tracking walker that needs to match up with the dispatch counter. The shader will send the following data to identify which wave to return the result to: The shader will send the following pipeline_ID to the ordered count unit to be used to select the correct pipeline's tracking data. Additionally pixel waves will use 4 counters depending on the packer sourcing the pixel waves and generating the launch order. Pipeline_id = ring_id + !pixel wave type; 0 = ring0 pixel wave 1 = ring0 CS 2 = ring1 CS 3 = ring2 CS Physical_wave_id =  {se_id, sh_id, wave_buf_id} GDS_size from the M0.size register contains the pkr_id (set at wave creation time) and logical_wave_id for pixel waves and launch order logical wave_id for compute shaders. The pixel shader uses four counters for each instance, so the pkr_id will need to be added to the gds_base to act on the correct counter. index =  find first valid (vector mask) count = src0[index][31:0]; Pkr_id =  gds_size[1:0]; gds_atomic_address[15:2]  = gds_base[15:2] will contain the dword address in the ds for the count accumulation counter. ds_address[15:2]  =  gds_base[15:2] + offset0[7:2]  + (pipeline_id == 0)?Pkr_id:0 //2 new control signals Wave_release = Offset1[0]; Wave_done = offset1[1]; Pixel_wave = offset1[2]; If this control is not set, hold the crawler until wave does an additional access with the wave_release the wave.  This feature
64	DS_ADD_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] += DATA[0:1]; RETURN_DATA[0:1] = tmp.
65	DS_SUB_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA[0:1]; RETURN_DATA[0:1] = tmp.
66	DS_RSUB_U64	Subtraction with reversed operands. // 64bit tmp = MEM[ADDR]; MEM[ADDR] = DATA - MEM[ADDR]; RETURN_DATA = tmp.
67	DS_INC_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA[0:1]) ? 0 : tmp + 1; // unsigned compare RETURN_DATA[0:1] = tmp.
68	DS_DEC_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA[0:1]) ? DATA[0:1] : tmp - 1; // unsigned compare RETURN_DATA[0:1] = tmp.
69	DS_MIN_I64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
70	DS_MAX_I64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
71	DS_MIN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
72	DS_MAX_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
73	DS_AND_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA[0:1]; RETURN_DATA[0:1] = tmp.
74	DS_OR_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA[0:1]; RETURN_DATA[0:1] = tmp.
75	DS_XOR_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA[0:1]; RETURN_DATA[0:1] = tmp.
76	DS_MSKOR_B64	Masked dword OR, D0 contains the mask and D1 contains the new value. // 64bit tmp = MEM[ADDR]; MEM[ADDR] = (MEM[ADDR] & ~DATA) | DATA2; RETURN_DATA = tmp.
77	DS_WRITE_B64	Write qword. // 64bit MEM[ADDR] = DATA.
78	DS_WRITE2_B64	Write 2 qwords. // 64bit MEM[ADDR + OFFSET0 * 8] = DATA; MEM[ADDR + OFFSET1 * 8] = DATA2.
79	DS_WRITE2ST64_B64	Write 2 qwords with a larger stride. // 64bit MEM[ADDR + OFFSET0 * 8 * 64] = DATA; MEM[ADDR + OFFSET1 * 8 * 64] = DATA2.
80	DS_CMPST_B64	Compare and store.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_CMPSWAP_X2 opcode. // 64bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
81	DS_CMPST_F64	Floating point compare and store that handles NaN/INF/denormal values.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_FCMPSWAP_X2 opcode. // 64bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
82	DS_MIN_F64	Floating point minimum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMIN_X2. // 64bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (cmp < tmp) ? src : tmp.
83	DS_MAX_F64	Floating point maximum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMAX_X2. // 64bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (tmp > cmp) ? src : tmp.
85	DS_ADD_RTN_F32	Floating point add that handles NaN/INF/denormal values. float tmp = MEM[ADDR].f32; MEM[ADDR].f32 += DATA0.f32; VDST.f32 = tmp;
96	DS_ADD_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] += DATA[0:1]; RETURN_DATA[0:1] = tmp.
97	DS_SUB_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA[0:1]; RETURN_DATA[0:1] = tmp.
98	DS_RSUB_RTN_U64	Subtraction with reversed operands. // 64bit tmp = MEM[ADDR]; MEM[ADDR] = DATA - MEM[ADDR]; RETURN_DATA = tmp.
99	DS_INC_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA[0:1]) ? 0 : tmp + 1; // unsigned compare RETURN_DATA[0:1] = tmp.
100	DS_DEC_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA[0:1]) ? DATA[0:1] : tmp - 1; // unsigned compare RETURN_DATA[0:1] = tmp.
101	DS_MIN_RTN_I64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
102	DS_MAX_RTN_I64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
103	DS_MIN_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
104	DS_MAX_RTN_U64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
105	DS_AND_RTN_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA[0:1]; RETURN_DATA[0:1] = tmp.
106	DS_OR_RTN_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA[0:1]; RETURN_DATA[0:1] = tmp.
107	DS_XOR_RTN_B64	// 64bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA[0:1]; RETURN_DATA[0:1] = tmp.
108	DS_MSKOR_RTN_B64	Masked dword OR, D0 contains the mask and D1 contains the new value. // 64bit tmp = MEM[ADDR]; MEM[ADDR] = (MEM[ADDR] & ~DATA) | DATA2; RETURN_DATA = tmp.
109	DS_WRXCHG_RTN_B64	Write-exchange operation. tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
110	DS_WRXCHG2_RTN_B64	 Write-exchange 2 separate qwords.
111	DS_WRXCHG2ST64_RTN_B64	 Write-exchange 2 qwords with a stride of 64 qwords.
112	DS_CMPST_RTN_B64	Compare and store.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_CMPSWAP_X2 opcode. // 64bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
113	DS_CMPST_RTN_F64	Floating point compare and store that handles NaN/INF/denormal values.  Caution, the order of src and cmp are the *opposite* of the BUFFER_ATOMIC_FCMPSWAP_X2 opcode. // 64bit tmp = MEM[ADDR]; src = DATA2; cmp = DATA; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
114	DS_MIN_RTN_F64	Floating point minimum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMIN_X2. // 64bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (cmp < tmp) ? src : tmp.
115	DS_MAX_RTN_F64	Floating point maximum that handles NaN/INF/denormal values. Note that this opcode is slightly more general-purpose than BUFFER_ATOMIC_FMAX_X2. // 64bit tmp = MEM[ADDR]; src = DATA; cmp = DATA2; MEM[ADDR] = (tmp > cmp) ? src : tmp.
118	DS_READ_B64	Read 1 qword. RETURN_DATA = MEM[ADDR].
119	DS_READ2_B64	Read 2 qwords. RETURN_DATA[0] = MEM[ADDR + OFFSET0 * 8]; RETURN_DATA[1] = MEM[ADDR + OFFSET1 * 8].
120	DS_READ2ST64_B64	Read 2 qwords with a larger stride. RETURN_DATA[0] = MEM[ADDR + OFFSET0 * 8 * 64]; RETURN_DATA[1] = MEM[ADDR + OFFSET1 * 8 * 64].
126	DS_CONDXCHG32_RTN_B64	 Conditional write exchange.
160	DS_WRITE_B8_D16_HI	Byte write in to high word. MEM[ADDR] = DATA[23:16].
161	DS_WRITE_B16_D16_HI	Short write in to high word. MEM[ADDR] = DATA[31:16].
162	DS_READ_U8_D16	Unsigned byte read with masked return to lower word. RETURN_DATA[15:0] = {8'h0,MEM[ADDR][7:0]}.
163	DS_READ_U8_D16_HI	Unsigned byte read with masked return to upper word. RETURN_DATA[31:16] = {8'h0,MEM[ADDR][7:0]}.
164	DS_READ_I8_D16	Signed byte read with masked return to lower word. RETURN_DATA[15:0] = signext(MEM[ADDR][7:0]).
165	DS_READ_I8_D16_HI	Signed byte read with masked return to upper word. RETURN_DATA[31:16] = signext(MEM[ADDR][7:0]).
166	DS_READ_U16_D16	Unsigned short read with masked return to lower word. RETURN_DATA[15:0] = MEM[ADDR][15:0].
167	DS_READ_U16_D16_HI	Unsigned short read with masked return to upper word. RETURN_DATA[31:0] = MEM[ADDR][15:0].
176	DS_WRITE_ADDTID_B32	Write dword with thread ID offset. LDS_GS[LDS_BASE + {OFFSET1,OFFSET0} + M0[15:0] + TID*4].u32 = VGPR[DATA0].u32
177	DS_READ_ADDTID_B32	Dword read with thread ID offset. VGPR[VDST].u32 = LDS_GS[LDS_BASE + {OFFSET1,OFFSET0} + M0[15:0] + TID*4].u32
178	DS_PERMUTE_B32	// VGPR[index][thread_id] is the VGPR RAM // VDST, ADDR and DATA0 are from the microcode DS encoding tmp[0..63] = 0 for i in 0..63 do // If a source thread is disabled, it will not propagate data. next if !EXEC[i] // ADDR needs to be divided by 4. // High-order bits are ignored. dst_lane = floor((VGPR[ADDR][i] + OFFSET) / 4) mod 64 tmp[dst_lane] = VGPR[DATA0][i] endfor // Copy data into destination VGPRs. If multiple sources // select the same destination thread, the highest-numbered // source thread wins. for i in 0..63 do next if !EXEC[i] VGPR[VDST][i] = tmp[i] endfor Forward permute. This does not access LDS memory and may be called even if no LDS memory is allocated to the wave.  It uses LDS hardware to implement an arbitrary swizzle across threads in a wavefront. Note the address passed in is the thread ID multiplied by 4. If multiple sources map to the same destination lane, the final value is not predictable but will be the value from one of the writers. See also DS_BPERMUTE_B32. Examples (simplified 4-thread wavefronts): VGPR[SRC0] = { A, B, C, D } VGPR[ADDR] = { 0, 0, 12, 4 } EXEC = 0xF, OFFSET = 0 VGPR[VDST] := { B, D, 0, C } VGPR[SRC0] = { A, B, C, D } VGPR[ADDR] = { 0, 0, 12, 4 } EXEC = 0xA, OFFSET = 0 VGPR[VDST] := { -, D, -, 0 }
179	DS_BPERMUTE_B32	// VGPR[index][thread_id] is the VGPR RAM // VDST, ADDR and DATA0 are from the microcode DS encoding tmp[0..63] = 0 for i in 0..63 do // ADDR needs to be divided by 4. // High-order bits are ignored. src_lane = floor((VGPR[ADDR][i] + OFFSET) / 4) mod 64 // EXEC is applied to the source VGPR reads. next if !EXEC[src_lane] tmp[i] = VGPR[DATA0][src_lane] endfor // Copy data into destination VGPRs. Some source // data may be broadcast to multiple lanes. for i in 0..63 do next if !EXEC[i] VGPR[VDST][i] = tmp[i] endfor Backward permute. This does not access LDS memory and may be called even if no LDS memory is allocated to the wave.  It uses LDS hardware to implement an arbitrary swizzle across threads in a wavefront. Note the address passed in is the thread ID multiplied by 4. Note that EXEC mask is applied to both VGPR read and write.  If src_lane selects a disabled thread, zero will be returned. See also DS_PERMUTE_B32. Examples (simplified 4-thread wavefronts): VGPR[SRC0] = { A, B, C, D } VGPR[ADDR] = { 0, 0, 12, 4 } EXEC = 0xF, OFFSET = 0 VGPR[VDST] := { A, A, D, B } VGPR[SRC0] = { A, B, C, D } VGPR[ADDR] = { 0, 0, 12, 4 } EXEC = 0xA, OFFSET = 0 VGPR[VDST] := { -, 0, -, B }
222	DS_WRITE_B96	Tri-dword write. {MEM[ADDR + 8], MEM[ADDR + 4], MEM[ADDR]} = DATA[95:0].
223	DS_WRITE_B128	Quad-dword write. {MEM[ADDR + 12], MEM[ADDR + 8], MEM[ADDR + 4], MEM[ADDR]} = DATA[127:0].
254	DS_READ_B96	Tri-dword read.
255	DS_READ_B128	Quad-dword read.

#  MUBUF Instructions
0	BUFFER_LOAD_FORMAT_X	Untyped buffer load 1 dword with format conversion.
1	BUFFER_LOAD_FORMAT_XY	Untyped buffer load 2 dwords with format conversion.
2	BUFFER_LOAD_FORMAT_XYZ	Untyped buffer load 3 dwords with format conversion.
3	BUFFER_LOAD_FORMAT_XYZW	Untyped buffer load 4 dwords with format conversion.
4	BUFFER_STORE_FORMAT_X	Untyped buffer store 1 dword with format conversion.
5	BUFFER_STORE_FORMAT_XY	Untyped buffer store 2 dwords with format conversion.
6	BUFFER_STORE_FORMAT_XYZ	Untyped buffer store 3 dwords with format conversion.
7	BUFFER_STORE_FORMAT_XYZW	Untyped buffer store 4 dwords with format conversion.
8	BUFFER_LOAD_UBYTE	Untyped buffer load unsigned byte (zero extend to VGPR destination).
9	BUFFER_LOAD_SBYTE	Untyped buffer load signed byte (sign extend to VGPR destination).
10	BUFFER_LOAD_USHORT	Untyped buffer load unsigned short (zero extend to VGPR destination).
11	BUFFER_LOAD_SSHORT	Untyped buffer load signed short (sign extend to VGPR destination).
12	BUFFER_LOAD_DWORD	Untyped buffer load dword.
13	BUFFER_LOAD_DWORDX2	Untyped buffer load 2 dwords.
14	BUFFER_LOAD_DWORDX4	Untyped buffer load 4 dwords.
15	BUFFER_LOAD_DWORDX3	Untyped buffer load 3 dwords.
24	BUFFER_STORE_BYTE	Untyped buffer store byte. Stores S0[7:0].
25	BUFFER_STORE_BYTE_D16_HI	Untyped buffer store byte. Stores S0[23:16].
26	BUFFER_STORE_SHORT	Untyped buffer store short. Stores S0[15:0].
27	BUFFER_STORE_SHORT_D16_HI	Untyped buffer store short. Stores S0[31:16].
28	BUFFER_STORE_DWORD	Untyped buffer store dword.
29	BUFFER_STORE_DWORDX2	Untyped buffer store 2 dwords.
30	BUFFER_STORE_DWORDX4	Untyped buffer store 4 dwords.
31	BUFFER_STORE_DWORDX3	Untyped buffer store 3 dwords.
32	BUFFER_LOAD_UBYTE_D16	D0[15:0] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
33	BUFFER_LOAD_UBYTE_D16_HI	D0[31:16] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
34	BUFFER_LOAD_SBYTE_D16	D0[15:0] = signext(MEM[ADDR]). Untyped buffer load signed byte.
35	BUFFER_LOAD_SBYTE_D16_HI	D0[31:16] = signext(MEM[ADDR]). Untyped buffer load signed byte.
36	BUFFER_LOAD_SHORT_D16	D0[15:0] = MEM[ADDR]. Untyped buffer load short.
37	BUFFER_LOAD_SHORT_D16_HI	D0[31:16] = MEM[ADDR]. Untyped buffer load short.
38	BUFFER_LOAD_FORMAT_D16_HI_X	 D0[31:16] = MEM[ADDR]. Untyped buffer load 1 dword with format conversion.
39	BUFFER_STORE_FORMAT_D16_HI_X	 Untyped buffer store 1 dword with format conversion.
48	BUFFER_ATOMIC_SWAP	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
49	BUFFER_ATOMIC_CMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
50	BUFFER_ATOMIC_ADD	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
51	BUFFER_ATOMIC_SUB	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
52	BUFFER_ATOMIC_CSUB	// 32bit old_value = MEM[ADDR]; if old_value < DATA then new_value = 0; else new_value = old_value - DATA; endif; MEM[addr] = new_value; RETURN_DATA = old_value.
53	BUFFER_ATOMIC_SMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
54	BUFFER_ATOMIC_UMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
55	BUFFER_ATOMIC_SMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
56	BUFFER_ATOMIC_UMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
57	BUFFER_ATOMIC_AND	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
58	BUFFER_ATOMIC_OR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
59	BUFFER_ATOMIC_XOR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
60	BUFFER_ATOMIC_INC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
61	BUFFER_ATOMIC_DEC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
62	BUFFER_ATOMIC_FCMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
63	BUFFER_ATOMIC_FMIN	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
64	BUFFER_ATOMIC_FMAX	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
80	BUFFER_ATOMIC_SWAP_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = DATA[0:1]; RETURN_DATA[0:1] = tmp.
81	BUFFER_ATOMIC_CMPSWAP_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0:1]; cmp = DATA[2:3]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0:1] = tmp.
82	BUFFER_ATOMIC_ADD_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] += DATA[0:1]; RETURN_DATA[0:1] = tmp.
83	BUFFER_ATOMIC_SUB_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA[0:1]; RETURN_DATA[0:1] = tmp.
85	BUFFER_ATOMIC_SMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
86	BUFFER_ATOMIC_UMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
87	BUFFER_ATOMIC_SMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
88	BUFFER_ATOMIC_UMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
89	BUFFER_ATOMIC_AND_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA[0:1]; RETURN_DATA[0:1] = tmp.
90	BUFFER_ATOMIC_OR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA[0:1]; RETURN_DATA[0:1] = tmp.
91	BUFFER_ATOMIC_XOR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA[0:1]; RETURN_DATA[0:1] = tmp.
92	BUFFER_ATOMIC_INC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA[0:1]) ? 0 : tmp + 1; // unsigned compare RETURN_DATA[0:1] = tmp.
93	BUFFER_ATOMIC_DEC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA[0:1]) ? DATA[0:1] : tmp - 1; // unsigned compare RETURN_DATA[0:1] = tmp.
94	BUFFER_ATOMIC_FCMPSWAP_X2	 // 64bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
95	BUFFER_ATOMIC_FMIN_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
96	BUFFER_ATOMIC_FMAX_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
113	BUFFER_GL0_INV	Write back and invalidate the shader L0.  Returns ACK to shader.
114	BUFFER_GL1_INV	Invalidate the GL1 cache only. Returns ACK to shader.
128	BUFFER_LOAD_FORMAT_D16_X	Untyped buffer load 1 dword with format conversion. D0[15:0] = MEM[ADDR].
129	BUFFER_LOAD_FORMAT_D16_XY	Untyped buffer load 1 dword with format conversion.
130	BUFFER_LOAD_FORMAT_D16_XY	Untyped buffer load 2 dwords with format conversion. Z
131	BUFFER_LOAD_FORMAT_D16_XY	Untyped buffer load 2 dwords with format conversion. ZW
132	BUFFER_STORE_FORMAT_D16_X	Untyped buffer store 1 dword with format conversion.
133	BUFFER_STORE_FORMAT_D16_	Untyped buffer store 1 dword with format conversion. XY
134	BUFFER_STORE_FORMAT_D16_	Untyped buffer store 2 dwords with format conversion. XYZ
135	BUFFER_STORE_FORMAT_D16_	Untyped buffer store 2 dwords with format conversion. XYZW

# MTBUF Instructions
0	TBUFFER_LOAD_FORMAT_X	Typed buffer load 1 dword with format conversion.
1	TBUFFER_LOAD_FORMAT_XY	Typed buffer load 2 dwords with format conversion.
2	TBUFFER_LOAD_FORMAT_XYZ	Typed buffer load 3 dwords with format conversion.
3	TBUFFER_LOAD_FORMAT_XYZW	Typed buffer load 4 dwords with format conversion.
4	TBUFFER_STORE_FORMAT_X	Typed buffer store 1 dword with format conversion.
5	TBUFFER_STORE_FORMAT_XY	Typed buffer store 2 dwords with format conversion.
6	TBUFFER_STORE_FORMAT_XYZ	Typed buffer store 3 dwords with format conversion.
7	TBUFFER_STORE_FORMAT_XYZW	Typed buffer store 4 dwords with format conversion.
8	TBUFFER_LOAD_FORMAT_D16_X	Typed buffer load 1 dword with format conversion.
9	TBUFFER_LOAD_FORMAT_D16_XY	Typed buffer load 1 dword with format conversion.
10	TBUFFER_LOAD_FORMAT_D16_XYZ	Typed buffer load 2 dwords with format conversion.
11	TBUFFER_LOAD_FORMAT_D16_XYZW	Typed buffer load 2 dwords with format conversion.
12	TBUFFER_STORE_FORMAT_D16_X	Typed buffer store 1 dword with format conversion.
13	TBUFFER_STORE_FORMAT_D16_XY	Typed buffer store 1 dword with format conversion.
14	TBUFFER_STORE_FORMAT_D16_XYZ	Typed buffer store 2 dwords with format conversion.
15	TBUFFER_STORE_FORMAT_D16_XYZW	Typed buffer store 2 dwords with format conversion.

# MIMG Instructions
0	IMAGE_LOAD	Load element from largest miplevel in resource view, with format conversion specified in the resource constant. No sampler.
1	IMAGE_LOAD_MIP	Load element from user-specified miplevel in resource view, with format conversion specified in the resource constant. No sampler.
2	IMAGE_LOAD_PCK	Load element from largest miplevel in resource view, without format conversion. 8- and 16-bit elements are not sign-extended. No sampler.
3	IMAGE_LOAD_PCK_SGN	Load element from largest miplevel in resource view, without format conversion. 8- and 16-bit elements are sign-extended. No sampler.
4	IMAGE_LOAD_MIP_PCK	Load element from user-supplied miplevel in resource view, without format conversion. 8- and 16-bit elements are not sign-extended. No sampler.
5	IMAGE_LOAD_MIP_PCK_SGN	Load element from user-supplied miplevel in resource view, without format conversion. 8- and 16-bit elements are sign-extended. No sampler.
8	IMAGE_STORE	Store element to largest miplevel in resource view, with format conversion specified in resource constant. No sampler.
9	IMAGE_STORE_MIP	Store element to user-specified miplevel in resource view, with format conversion specified in resource constant. No sampler.
10	IMAGE_STORE_PCK	Store element to largest miplevel in resource view, without format conversion.  No sampler.
11	IMAGE_STORE_MIP_PCK	Store element to user-specified miplevel in resource view, without format conversion.  No sampler.
14	IMAGE_GET_RESINFO	Return resource info for a given mip level specified in the address vgpr. No sampler. Returns 4 integer values into VGPRs 3-0: {num_mip_levels, depth, height, width}.
15	IMAGE_ATOMIC_SWAP	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
16	IMAGE_ATOMIC_CMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
17	IMAGE_ATOMIC_ADD	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
18	IMAGE_ATOMIC_SUB	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
20	IMAGE_ATOMIC_SMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
21	IMAGE_ATOMIC_UMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
22	IMAGE_ATOMIC_SMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
23	IMAGE_ATOMIC_UMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
24	IMAGE_ATOMIC_AND	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
25	IMAGE_ATOMIC_OR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
26	IMAGE_ATOMIC_XOR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
27	IMAGE_ATOMIC_INC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
28	IMAGE_ATOMIC_DEC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
29	IMAGE_ATOMIC_FCMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
30	IMAGE_ATOMIC_FMIN	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
31	IMAGE_ATOMIC_FMAX	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
32	IMAGE_SAMPLE	sample texture map.
33	IMAGE_SAMPLE_CL	sample texture map, with LOD clamp specified in shader.
34	IMAGE_SAMPLE_D	sample texture map, with user derivatives
35	IMAGE_SAMPLE_D_CL	sample texture map, with LOD clamp specified in shader, with user derivatives.
36	IMAGE_SAMPLE_L	sample texture map, with user LOD.
37	IMAGE_SAMPLE_B	sample texture map, with lod bias.
38	IMAGE_SAMPLE_B_CL	sample texture map, with LOD clamp specified in shader, with lod bias.
39	IMAGE_SAMPLE_LZ	sample texture map, from level 0.
40	IMAGE_SAMPLE_C	sample texture map, with PCF.
41	IMAGE_SAMPLE_C_CL	SAMPLE_C, with LOD clamp specified in shader.
42	IMAGE_SAMPLE_C_D	SAMPLE_C, with user derivatives.
43	IMAGE_SAMPLE_C_D_CL	SAMPLE_C, with LOD clamp specified in shader, with user derivatives.
44	IMAGE_SAMPLE_C_L	SAMPLE_C, with user LOD.
45	IMAGE_SAMPLE_C_B	SAMPLE_C, with lod bias.
46	IMAGE_SAMPLE_C_B_CL	SAMPLE_C, with LOD clamp specified in shader, with lod bias.
47	IMAGE_SAMPLE_C_LZ	SAMPLE_C, from level 0.
48	IMAGE_SAMPLE_O	sample texture map, with user offsets.
49	IMAGE_SAMPLE_CL_O	SAMPLE_O with LOD clamp specified in shader.
50	IMAGE_SAMPLE_D_O	SAMPLE_O, with user derivatives.
51	IMAGE_SAMPLE_D_CL_O	SAMPLE_O, with LOD clamp specified in shader, with user derivatives.
52	IMAGE_SAMPLE_L_O	SAMPLE_O, with user LOD.
53	IMAGE_SAMPLE_B_O	SAMPLE_O, with lod bias.
54	IMAGE_SAMPLE_B_CL_O	SAMPLE_O, with LOD clamp specified in shader, with lod bias.
55	IMAGE_SAMPLE_LZ_O	SAMPLE_O, from level 0.
56	IMAGE_SAMPLE_C_O	SAMPLE_C with user specified offsets.
57	IMAGE_SAMPLE_C_CL_O	SAMPLE_C_O, with LOD clamp specified in shader.
58	IMAGE_SAMPLE_C_D_O	SAMPLE_C_O, with user derivatives.
59	IMAGE_SAMPLE_C_D_CL_O	SAMPLE_C_O, with LOD clamp specified in shader, with user derivatives.
60	IMAGE_SAMPLE_C_L_O	SAMPLE_C_O, with user LOD.
61	IMAGE_SAMPLE_C_B_O	SAMPLE_C_O, with lod bias.
62	IMAGE_SAMPLE_C_B_CL_O	SAMPLE_C_O, with LOD clamp specified in shader, with lod bias.
63	IMAGE_SAMPLE_C_LZ_O	SAMPLE_C_O, from level 0.
64	IMAGE_GATHER4	gather 4 single component elements (2x2).
65	IMAGE_GATHER4_CL	gather 4 single component elements (2x2) with user LOD clamp.
68	IMAGE_GATHER4_L	gather 4 single component elements (2x2) with user LOD.
69	IMAGE_GATHER4_B	gather 4 single component elements (2x2) with user bias.
70	IMAGE_GATHER4_B_CL	gather 4 single component elements (2x2) with user bias and clamp.
71	IMAGE_GATHER4_LZ	gather 4 single component elements (2x2) at level 0.
72	IMAGE_GATHER4_C	gather 4 single component elements (2x2) with PCF.
73	IMAGE_GATHER4_C_CL	gather 4 single component elements (2x2) with user LOD clamp and PCF.
76	IMAGE_GATHER4_C_L	gather 4 single component elements (2x2) with user LOD and PCF.
77	IMAGE_GATHER4_C_B	gather 4 single component elements (2x2) with user bias and PCF.
78	IMAGE_GATHER4_C_B_CL	gather 4 single component elements (2x2) with user bias, clamp and PCF.
79	IMAGE_GATHER4_C_LZ	gather 4 single component elements (2x2) at level 0, with PCF.
80	IMAGE_GATHER4_O	GATHER4, with user offsets.
81	IMAGE_GATHER4_CL_O	GATHER4_CL, with user offsets.
84	IMAGE_GATHER4_L_O	GATHER4_L, with user offsets.
85	IMAGE_GATHER4_B_O	GATHER4_B, with user offsets.
86	IMAGE_GATHER4_B_CL_O	GATHER4_B_CL, with user offsets.
87	IMAGE_GATHER4_LZ_O	GATHER4_LZ, with user offsets.
88	IMAGE_GATHER4_C_O	GATHER4_C, with user offsets.
89	IMAGE_GATHER4_C_CL_O	GATHER4_C_CL, with user offsets.
92	IMAGE_GATHER4_C_L_O	GATHER4_C_L, with user offsets.
93	IMAGE_GATHER4_C_B_O	GATHER4_B, with user offsets.
94	IMAGE_GATHER4_C_B_CL_O	GATHER4_B_CL, with user offsets.
95	IMAGE_GATHER4_C_LZ_O	GATHER4_C_LZ, with user offsets.
96	IMAGE_GET_LOD	VDATA[0] = clampedLOD; VDATA[1] = rawLOD. Return calculated LOD as two 32-bit floating point values.
97	IMAGE_GATHER4H	Fetch 1 component per texel from 4x1 texels.  DMASK selects which component to read (R,G,B,A) and must have only one bit set to 1.
128	IMAGE_MSAA_LOAD	Load up to 4 samples of 1 component from an MSAA resource with a user-specified fragment ID. No sampler.
162	IMAGE_SAMPLE_D_G16	SAMPLE_D with 16-bit floating point derivatives (gradients)
163	IMAGE_SAMPLE_D_CL_G16	SAMPLE_D_CL with 16-bit floating point derivatives (gradients)
170	IMAGE_SAMPLE_C_D_G16	SAMPLE_C_D with 16-bit floating point derivatives (gradients)
171	IMAGE_SAMPLE_C_D_CL_G16	SAMPLE_C_D_CL with 16-bit floating point derivatives (gradients)
178	IMAGE_SAMPLE_D_O_G16	SAMPLE_D_O with 16-bit floating point derivatives (gradients)
179	IMAGE_SAMPLE_D_CL_O_G16	SAMPLE_D_CL_O with 16-bit floating point derivatives (gradients)
186	IMAGE_SAMPLE_C_D_O_G16	SAMPLE_C_D_O with 16-bit floating point derivatives (gradients)
187	IMAGE_SAMPLE_C_D_CL_O_G16	 SAMPLE_C_D_CL_O with 16-bit floating point derivatives (gradients)
230	IMAGE_BVH_INTERSECT_RAY	Intersection test on bound volume hierarchy nodes for ray tracing acceleration.  32-bit node pointer.  No sampler. DATA: The destination VGPRs contain the results of intersection testing.  The values returned here are different depending on the type of BVH node that was fetched. For box nodes the results contain the 4 pointers of the children boxes in intersection time sorted order. For triangle BVH nodes the results contain the intersection time and triangle ID of the triangle tested. ADDR: 11 address VGPRs contain the ray data and BVH node pointer for the intersection test.  The data is laid out as follows: vgpr_a[0] = node_pointer (uint32) vgpr_a[1] = ray_extent (float32) vgpr_a[2] = ray_origin.x (float32) vgpr_a[3] = ray_origin.y (float32) vgpr_a[4] = ray_origin.z (float32) vgpr_a[5] = ray_dir.x (float32) vgpr_a[6] = ray_dir.y (float32) vgpr_a[7] = ray_dir.z (float32) vgpr_a[8] = ray_inv_dir.x (float32) vgpr_a[9] = ray_inv_dir.y (float32) vgpr_a[10]= ray_inv_dir.z (float32) For performance and power optimization, the instruction can be encoded to use 16 bit floats for ray_dir and ray_inv_dir by setting A16 to 1. When the instruction is encoded with 16 bit addresses only 8 address VGPRs are used as follows: vgpr_a[0] = node_pointer (uint32) vgpr_a[1] = ray_extent (float32) vgpr_a[2] = ray_origin.x (float32) vgpr_a[3] = ray_origin.y (float32) vgpr_a[4] = ray_origin.z (float32) vgpr_a[5] = {ray_dir.x,ray_dir.y}(2x float16) vgpr_a[6] = {ray_dir.z,ray_inv_dir.x}(2x float16) vgpr_a[7] = {ray_inv_dir.y,ray_inv_dir.z}(2x float16) RSRC: The resource is the texture descriptor for the operation.  The instruction must e encoded with r128=1. RESTRICTIONS: The image_bvh_intersect_ray and image_bvh64_intersect_ray opcode do not support all of

#  Flat Instructions
8	FLAT_LOAD_UBYTE	Untyped buffer load unsigned byte (zero extend to VGPR destination).
9	FLAT_LOAD_SBYTE	Untyped buffer load signed byte (sign extend to VGPR destination).
10	FLAT_LOAD_USHORT	Untyped buffer load unsigned short (zero extend to VGPR destination).
11	FLAT_LOAD_SSHORT	Untyped buffer load signed short (sign extend to VGPR destination).
12	FLAT_LOAD_DWORD	Untyped buffer load dword.
13	FLAT_LOAD_DWORDX2	Untyped buffer load 2 dwords.
14	FLAT_LOAD_DWORDX4	Untyped buffer load 4 dwords.
15	FLAT_LOAD_DWORDX3	Untyped buffer load 3 dwords.
24	FLAT_STORE_BYTE	Untyped buffer store byte. Stores S0[7:0].
25	FLAT_STORE_BYTE_D16_HI	Untyped buffer store byte. Stores S0[23:16].
26	FLAT_STORE_SHORT	Untyped buffer store short. Stores S0[15:0].
27	FLAT_STORE_SHORT_D16_HI	Untyped buffer store short. Stores S0[31:16].
28	FLAT_STORE_DWORD	Untyped buffer store dword.
29	FLAT_STORE_DWORDX2	Untyped buffer store 2 dwords.
30	FLAT_STORE_DWORDX4	Untyped buffer store 4 dwords.
31	FLAT_STORE_DWORDX3	Untyped buffer store 3 dwords.
32	FLAT_LOAD_UBYTE_D16	D0[15:0] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
33	FLAT_LOAD_UBYTE_D16_HI	D0[31:16] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
34	FLAT_LOAD_SBYTE_D16	D0[15:0] = signext(MEM[ADDR]). Untyped buffer load signed byte.
35	FLAT_LOAD_SBYTE_D16_HI	D0[31:16] = signext(MEM[ADDR]). Untyped buffer load signed byte.
36	FLAT_LOAD_SHORT_D16	D0[15:0] = MEM[ADDR]. Untyped buffer load short.
37	FLAT_LOAD_SHORT_D16_HI	D0[31:16] = MEM[ADDR]. Untyped buffer load short.
48	FLAT_ATOMIC_SWAP	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
49	FLAT_ATOMIC_CMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
50	FLAT_ATOMIC_ADD	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
51	FLAT_ATOMIC_SUB	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
53	FLAT_ATOMIC_SMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
54	FLAT_ATOMIC_UMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
55	FLAT_ATOMIC_SMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
56	FLAT_ATOMIC_UMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
57	FLAT_ATOMIC_AND	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
58	FLAT_ATOMIC_OR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
59	FLAT_ATOMIC_XOR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
60	FLAT_ATOMIC_INC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
61	FLAT_ATOMIC_DEC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
62	FLAT_ATOMIC_FCMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
63	FLAT_ATOMIC_FMIN	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
64	FLAT_ATOMIC_FMAX	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
80	FLAT_ATOMIC_SWAP_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = DATA[0:1]; RETURN_DATA[0:1] = tmp.
81	FLAT_ATOMIC_CMPSWAP_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0:1]; cmp = DATA[2:3]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0:1] = tmp.
82	FLAT_ATOMIC_ADD_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] += DATA[0:1]; RETURN_DATA[0:1] = tmp.
83	FLAT_ATOMIC_SUB_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA[0:1]; RETURN_DATA[0:1] = tmp.
85	FLAT_ATOMIC_SMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
86	FLAT_ATOMIC_UMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
87	FLAT_ATOMIC_SMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
88	FLAT_ATOMIC_UMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
89	FLAT_ATOMIC_AND_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA[0:1]; RETURN_DATA[0:1] = tmp.
90	FLAT_ATOMIC_OR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA[0:1]; RETURN_DATA[0:1] = tmp.
91	FLAT_ATOMIC_XOR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA[0:1]; RETURN_DATA[0:1] = tmp.
92	FLAT_ATOMIC_INC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA[0:1]) ? 0 : tmp + 1; // unsigned compare RETURN_DATA[0:1] = tmp.
93	FLAT_ATOMIC_DEC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA[0:1]) ? DATA[0:1] : tmp - 1; // unsigned compare RETURN_DATA[0:1] = tmp.
94	FLAT_ATOMIC_FCMPSWAP_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
95	FLAT_ATOMIC_FMIN_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
96	FLAT_ATOMIC_FMAX_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).

#  Scratch Instructions
8	SCRATCH_LOAD_UBYTE	Untyped buffer load unsigned byte (zero extend to VGPR destination).
9	SCRATCH_LOAD_SBYTE	Untyped buffer load signed byte (sign extend to VGPR destination).
10	SCRATCH_LOAD_USHORT	Untyped buffer load unsigned short (zero extend to VGPR destination).
11	SCRATCH_LOAD_SSHORT	Untyped buffer load signed short (sign extend to VGPR destination).
12	SCRATCH_LOAD_DWORD	Untyped buffer load dword.
13	SCRATCH_LOAD_DWORDX2	Untyped buffer load 2 dwords.
14	SCRATCH_LOAD_DWORDX4	Untyped buffer load 4 dwords.
15	SCRATCH_LOAD_DWORDX3	Untyped buffer load 3 dwords.
24	SCRATCH_STORE_BYTE	Untyped buffer store byte. Stores S0[7:0].
25	SCRATCH_STORE_BYTE_D16_HI	Untyped buffer store byte. Stores S0[23:16].
26	SCRATCH_STORE_SHORT	Untyped buffer store short. Stores S0[15:0].
27	SCRATCH_STORE_SHORT_D16_HI	Untyped buffer store short. Stores S0[31:16].
28	SCRATCH_STORE_DWORD	Untyped buffer store dword.
29	SCRATCH_STORE_DWORDX2	Untyped buffer store 2 dwords.
30	SCRATCH_STORE_DWORDX4	Untyped buffer store 4 dwords.
31	SCRATCH_STORE_DWORDX3	Untyped buffer store 3 dwords.
32	SCRATCH_LOAD_UBYTE_D16	D0[15:0] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
33	SCRATCH_LOAD_UBYTE_D16_HI	D0[31:16] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
34	SCRATCH_LOAD_SBYTE_D16	D0[15:0] = signext(MEM[ADDR]). Untyped buffer load signed byte.
35	SCRATCH_LOAD_SBYTE_D16_HI	D0[31:16] = signext(MEM[ADDR]). Untyped buffer load signed byte.
36	SCRATCH_LOAD_SHORT_D16	D0[15:0] = MEM[ADDR]. Untyped buffer load short.
37	SCRATCH_LOAD_SHORT_D16_HI	D0[31:16] = MEM[ADDR]. Untyped buffer load short.

# Global Instructions
8	GLOBAL_LOAD_UBYTE	Untyped buffer load unsigned byte (zero extend to VGPR destination).
9	GLOBAL_LOAD_SBYTE	Untyped buffer load signed byte (sign extend to VGPR destination).
10	GLOBAL_LOAD_USHORT	Untyped buffer load unsigned short (zero extend to VGPR destination).
11	GLOBAL_LOAD_SSHORT	Untyped buffer load signed short (sign extend to VGPR destination).
12	GLOBAL_LOAD_DWORD	Untyped buffer load dword.
13	GLOBAL_LOAD_DWORDX2	Untyped buffer load 2 dwords.
14	GLOBAL_LOAD_DWORDX4	Untyped buffer load 4 dwords.
15	GLOBAL_LOAD_DWORDX3	Untyped buffer load 3 dwords.
22	GLOBAL_LOAD_DWORD_ADDTI	Untyped buffer load dword.  No VGPR address is supplied D in this instruction.  TID is added to the address as shown below: memory_Addr = sgpr_addr(64) + inst_offset(12) + tid*4
23	GLOBAL_STORE_DWORD_ADD	Untyped buffer store dword.  No VGPR address is supplied TID in this instruction.  TID is added to the address as shown below: memory_Addr = sgpr_addr(64) + inst_offset(12) + tid*4
24	GLOBAL_STORE_BYTE	Untyped buffer store byte. Stores S0[7:0].
25	GLOBAL_STORE_BYTE_D16_HI	Untyped buffer store byte. Stores S0[23:16].
26	GLOBAL_STORE_SHORT	Untyped buffer store short. Stores S0[15:0].
27	GLOBAL_STORE_SHORT_D16_	Untyped buffer store short. Stores S0[31:16]. HI
28	GLOBAL_STORE_DWORD	Untyped buffer store dword.
29	GLOBAL_STORE_DWORDX2	Untyped buffer store 2 dwords.
30	GLOBAL_STORE_DWORDX4	Untyped buffer store 4 dwords.
31	GLOBAL_STORE_DWORDX3	Untyped buffer store 3 dwords.
32	GLOBAL_LOAD_UBYTE_D16	D0[15:0] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
33	GLOBAL_LOAD_UBYTE_D16_HI	D0[31:16] = {8'h0, MEM[ADDR]}. Untyped buffer load unsigned byte.
34	GLOBAL_LOAD_SBYTE_D16	D0[15:0] = signext(MEM[ADDR]). Untyped buffer load signed byte.
35	GLOBAL_LOAD_SBYTE_D16_HI	D0[31:16] = signext(MEM[ADDR]). Untyped buffer load signed byte.
36	GLOBAL_LOAD_SHORT_D16	D0[15:0] = MEM[ADDR]. Untyped buffer load short.
37	GLOBAL_LOAD_SHORT_D16_HI	D0[31:16] = MEM[ADDR]. Untyped buffer load short.
48	GLOBAL_ATOMIC_SWAP	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = DATA; RETURN_DATA = tmp.
49	GLOBAL_ATOMIC_CMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp.
50	GLOBAL_ATOMIC_ADD	// 32bit tmp = MEM[ADDR]; MEM[ADDR] += DATA; RETURN_DATA = tmp.
51	GLOBAL_ATOMIC_SUB	// 32bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA; RETURN_DATA = tmp.
52	GLOBAL_ATOMIC_CSUB	// 32bit old_value = MEM[ADDR]; if old_value < DATA then new_value = 0; else new_value = old_value - DATA; endif; MEM[addr] = new_value; RETURN_DATA = old_value.
53	GLOBAL_ATOMIC_SMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
54	GLOBAL_ATOMIC_UMIN	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA < tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
55	GLOBAL_ATOMIC_SMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // signed compare RETURN_DATA = tmp.
56	GLOBAL_ATOMIC_UMAX	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA > tmp) ? DATA : tmp; // unsigned compare RETURN_DATA = tmp.
57	GLOBAL_ATOMIC_AND	// 32bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA; RETURN_DATA = tmp.
58	GLOBAL_ATOMIC_OR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA; RETURN_DATA = tmp.
59	GLOBAL_ATOMIC_XOR	// 32bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA; RETURN_DATA = tmp.
60	GLOBAL_ATOMIC_INC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA) ? 0 : tmp + 1; // unsigned compare RETURN_DATA = tmp.
61	GLOBAL_ATOMIC_DEC	// 32bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA) ? DATA : tmp - 1; // unsigned compare RETURN_DATA = tmp.
62	GLOBAL_ATOMIC_FCMPSWAP	// 32bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
63	GLOBAL_ATOMIC_FMIN	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
64	GLOBAL_ATOMIC_FMAX	// 32bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src > tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
80	GLOBAL_ATOMIC_SWAP_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = DATA[0:1]; RETURN_DATA[0:1] = tmp.
81	GLOBAL_ATOMIC_CMPSWAP_X2	 // 64bit tmp = MEM[ADDR]; src = DATA[0:1]; cmp = DATA[2:3]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0:1] = tmp.
82	GLOBAL_ATOMIC_ADD_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] += DATA[0:1]; RETURN_DATA[0:1] = tmp.
83	GLOBAL_ATOMIC_SUB_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] -= DATA[0:1]; RETURN_DATA[0:1] = tmp.
85	GLOBAL_ATOMIC_SMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
86	GLOBAL_ATOMIC_UMIN_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] < tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
87	GLOBAL_ATOMIC_SMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // signed compare RETURN_DATA[0:1] = tmp.
88	GLOBAL_ATOMIC_UMAX_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (DATA[0:1] > tmp) ? DATA[0:1] : tmp; // unsigned compare RETURN_DATA[0:1] = tmp.
89	GLOBAL_ATOMIC_AND_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] &= DATA[0:1]; RETURN_DATA[0:1] = tmp.
90	GLOBAL_ATOMIC_OR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] |= DATA[0:1]; RETURN_DATA[0:1] = tmp.
91	GLOBAL_ATOMIC_XOR_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] ^= DATA[0:1]; RETURN_DATA[0:1] = tmp.
92	GLOBAL_ATOMIC_INC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp >= DATA[0:1]) ? 0 : tmp + 1; // unsigned compare RETURN_DATA[0:1] = tmp.
93	GLOBAL_ATOMIC_DEC_X2	// 64bit tmp = MEM[ADDR]; MEM[ADDR] = (tmp == 0 || tmp > DATA[0:1]) ? DATA[0:1] : tmp - 1; // unsigned compare RETURN_DATA[0:1] = tmp.
94	GLOBAL_ATOMIC_FCMPSWAP_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; cmp = DATA[1]; MEM[ADDR] = (tmp == cmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare swap (handles NaN/INF/denorm).
95	GLOBAL_ATOMIC_FMIN_X2	// 64bit tmp = MEM[ADDR]; src = DATA[0]; MEM[ADDR] = (src < tmp) ? src : tmp; RETURN_DATA[0] = tmp. Floating-point compare (handles NaN/INF/denorm).
